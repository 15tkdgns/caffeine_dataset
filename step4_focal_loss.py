"""
STEP 4: Focal Loss ì ìš©
- ì–´ë ¤ìš´ ìƒ˜í”Œ(ìƒí™œ ì¹´í…Œê³ ë¦¬)ì— ì§‘ì¤‘
- XGBoost + LightGBMì— Focal Loss ì ìš©
- Î³ = 2.0 ì‚¬ìš©
"""

import numpy as np
import xgboost as xgb
import lightgbm as lgb
from sklearn.metrics import accuracy_score, f1_score
import time
import joblib
import json
from datetime import datetime

print("="*80)
print("ğŸ”¥ STEP 4: Focal Loss ì ìš©")
print("="*80)

# ============================================================
# 1. Focal Loss êµ¬í˜„
# ============================================================
print("\n[1/5] Focal Loss êµ¬í˜„")

def focal_loss_lgb(y_true, y_pred, gamma=2.0, alpha=None):
    """
    LightGBMìš© Focal Loss
    Î³ (gamma): focusing parameter (default=2.0)
    """
    # y_predëŠ” raw score (logits)
    # Softmax ì ìš©
    exp_preds = np.exp(y_pred - np.max(y_pred, axis=1, keepdims=True))
    probs = exp_preds / np.sum(exp_preds, axis=1, keepdims=True)
    
    # One-hot encoding
    n_samples = y_true.shape[0]
    n_classes = probs.shape[1]
    y_true_one_hot = np.zeros((n_samples, n_classes))
    y_true_one_hot[np.arange(n_samples), y_true.astype(int)] = 1
    
    # Focal loss gradient
    p_t = np.sum(probs * y_true_one_hot, axis=1, keepdims=True)
    grad = probs - y_true_one_hot
    grad = grad * (1 - p_t) ** (gamma - 1) * (gamma * p_t * np.log(p_t + 1e-15) + p_t - 1)
    
    # Hessian (approximation)
    hess = probs * (1 - probs) * (1 - p_t) ** gamma
    
    return grad.flatten(), hess.flatten()

print(f"  âœ… Focal Loss (Î³=2.0) ì¤€ë¹„ ì™„ë£Œ")

# ============================================================
# 2. ë°ì´í„° ë¡œë“œ
# ============================================================
print("\n[2/5] ë°ì´í„° ë¡œë“œ")

X_train = np.load('02_data/02_augmented/X_train_smote.npy')
y_train = np.load('02_data/02_augmented/y_train_smote.npy')
X_test = np.load('02_data/02_augmented/X_test.npy')
y_test = np.load('02_data/02_augmented/y_test.npy')

# STEP 2 ì „ëµì  ì–¸ë”ìƒ˜í”Œë§ ì ìš©
sampling_ratios = {0: 1.0, 1: 1.0, 2: 0.7, 3: 0.7, 4: 0.7, 5: 1.0}

indices_to_keep = []
for class_id in range(6):
    class_mask = (y_train == class_id)
    class_indices = np.where(class_mask)[0]
    n_samples = len(class_indices)
    n_keep = int(n_samples * sampling_ratios[class_id])
    np.random.seed(42)
    kept_indices = np.random.choice(class_indices, n_keep, replace=False)
    indices_to_keep.extend(kept_indices)

X_train_sampled = X_train[np.array(indices_to_keep)]
y_train_sampled = y_train[np.array(indices_to_keep)]

print(f"  í•™ìŠµ ë°ì´í„°: {len(X_train_sampled):,}ê±´")
print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test):,}ê±´")

category_names = ['êµí†µ', 'ìƒí™œ', 'ì‡¼í•‘', 'ì‹ë£Œí’ˆ', 'ì™¸ì‹', 'ì£¼ìœ ']

# ============================================================
# 3. XGBoost with Focal Loss (Custom Objective)
# ============================================================
print("\n[3/5] XGBoost with Focal Loss")

def focal_loss_xgb(preds, dtrain, gamma=2.0):
    """XGBoostìš© Focal Loss"""
    labels = dtrain.get_label()
    n_classes = 6
    
    # Reshape predictions
    preds = preds.reshape(len(labels), n_classes)
    
    # Softmax
    exp_preds = np.exp(preds - np.max(preds, axis=1, keepdims=True))
    probs = exp_preds / np.sum(exp_preds, axis=1, keepdims=True)
    
    # One-hot
    n_samples = len(labels)
    y_one_hot = np.zeros((n_samples, n_classes))
    y_one_hot[np.arange(n_samples), labels.astype(int)] = 1
    
    # Gradient
    p_t = np.sum(probs * y_one_hot, axis=1, keepdims=True)
    grad = probs - y_one_hot
    grad = grad * (1 - p_t) ** (gamma - 1) * (gamma * p_t * np.log(p_t + 1e-15) + p_t - 1)
    
    # Hessian
    hess = probs * (1 - probs) * (1 - p_t) ** gamma
    
    return grad.flatten(), hess.flatten()

# ì¼ë°˜ XGBoost (ë¹„êµìš©)
print("  [XGBoost ì¼ë°˜] í•™ìŠµ ì‹œì‘...")
start = time.time()

xgb_normal = xgb.XGBClassifier(
    device='cuda',
    tree_method='hist',
    n_estimators=300,
    max_depth=10,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)

xgb_normal.fit(X_train_sampled, y_train_sampled)
train_time_xgb_normal = time.time() - start

y_pred_xgb_normal = xgb_normal.predict(X_test)
acc_xgb_normal = accuracy_score(y_test, y_pred_xgb_normal)
f1_xgb_normal = f1_score(y_test, y_pred_xgb_normal, average='macro')
cat_f1_xgb_normal = f1_score(y_test, y_pred_xgb_normal, average=None)

print(f"  âœ… ì™„ë£Œ: {train_time_xgb_normal:.2f}ì´ˆ")
print(f"     Accuracy: {acc_xgb_normal:.4f}, Macro F1: {f1_xgb_normal:.4f}, ìƒí™œ F1: {cat_f1_xgb_normal[1]:.4f}")

# XGBoost with Focal Loss
print("\n  [XGBoost Focal Loss] í•™ìŠµ ì‹œì‘...")
start = time.time()

dtrain = xgb.DMatrix(X_train_sampled, label=y_train_sampled)
dtest = xgb.DMatrix(X_test, label=y_test)

params = {
    'device': 'cuda',
    'tree_method': 'hist',
    'max_depth': 10,
    'learning_rate': 0.05,  # Focal LossëŠ” ë‚®ì€ LR ê¶Œì¥
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'num_class': 6,
    'seed': 42
}

xgb_focal = xgb.train(
    params,
    dtrain,
    num_boost_round=300,
    obj=focal_loss_xgb,
    verbose_eval=False
)

train_time_xgb_focal = time.time() - start

# ì˜ˆì¸¡
y_pred_probs_focal = xgb_focal.predict(dtest)
y_pred_xgb_focal = np.argmax(y_pred_probs_focal, axis=1)

acc_xgb_focal = accuracy_score(y_test, y_pred_xgb_focal)
f1_xgb_focal = f1_score(y_test, y_pred_xgb_focal, average='macro')
cat_f1_xgb_focal = f1_score(y_test, y_pred_xgb_focal, average=None)

print(f"  âœ… ì™„ë£Œ: {train_time_xgb_focal:.2f}ì´ˆ")
print(f"     Accuracy: {acc_xgb_focal:.4f}, Macro F1: {f1_xgb_focal:.4f}, ìƒí™œ F1: {cat_f1_xgb_focal[1]:.4f}")

# ============================================================
# 4. LightGBM (ì¼ë°˜ + Class Weight)
# ============================================================
print("\n[4/5] LightGBM with Enhanced Class Weight")

# Class Weight ê³„ì‚°
from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight('balanced', classes=np.unique(y_train_sampled), y=y_train_sampled)
class_weights[1] *= 2.5  # ìƒí™œ ì¹´í…Œê³ ë¦¬ ê°•í™”

sample_weights = np.array([class_weights[y] for y in y_train_sampled])

print("  í•™ìŠµ ì‹œì‘...")
start = time.time()

lgb_model = lgb.LGBMClassifier(
    n_estimators=300,
    max_depth=10,
    learning_rate=0.1,
    num_leaves=128,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
    verbose=-1
)

lgb_model.fit(X_train_sampled, y_train_sampled, sample_weight=sample_weights)
train_time_lgb = time.time() - start

y_pred_lgb = lgb_model.predict(X_test)
acc_lgb = accuracy_score(y_test, y_pred_lgb)
f1_lgb = f1_score(y_test, y_pred_lgb, average='macro')
cat_f1_lgb = f1_score(y_test, y_pred_lgb, average=None)

print(f"  âœ… ì™„ë£Œ: {train_time_lgb:.2f}ì´ˆ")
print(f"     Accuracy: {acc_lgb:.4f}, Macro F1: {f1_lgb:.4f}, ìƒí™œ F1: {cat_f1_lgb[1]:.4f}")

# ============================================================
# 5. ê²°ê³¼ ì €ì¥ ë° ë¹„êµ
# ============================================================
print("\n[5/5] ê²°ê³¼ ì €ì¥")

import os
os.makedirs('04_logs/step4_focal_loss', exist_ok=True)

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
best_scores = {
    'xgb_normal': acc_xgb_normal,
    'xgb_focal': acc_xgb_focal,
    'lgb': acc_lgb
}
best_model_name = max(best_scores, key=best_scores.get)

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# ë©”íƒ€ë°ì´í„°
metadata = {
    'experiment': 'STEP 4: Focal Loss',
    'results': {
        'xgb_normal': {
            'accuracy': float(acc_xgb_normal),
            'macro_f1': float(f1_xgb_normal),
            'living_f1': float(cat_f1_xgb_normal[1]),
            'category_f1': {cat: float(f1) for cat, f1 in zip(category_names, cat_f1_xgb_normal)}
        },
        'xgb_focal_loss': {
            'accuracy': float(acc_xgb_focal),
            'macro_f1': float(f1_xgb_focal),
            'living_f1': float(cat_f1_xgb_focal[1]),
            'category_f1': {cat: float(f1) for cat, f1 in zip(category_names, cat_f1_xgb_focal)}
        },
        'lgb_weighted': {
            'accuracy': float(acc_lgb),
            'macro_f1': float(f1_lgb),
            'living_f1': float(cat_f1_lgb[1]),
            'category_f1': {cat: float(f1) for cat, f1 in zip(category_names, cat_f1_lgb)}
        }
    }
}

with open(f'04_logs/step4_focal_loss/metadata_{timestamp}.json', 'w', encoding='utf-8') as f:
    json.dump(metadata, f, indent=2, ensure_ascii=False)

print(f"  âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")

# ============================================================
# ìµœì¢… ë¹„êµ
# ============================================================
print("\n" + "="*80)
print("ğŸ† STEP 4 ê²°ê³¼ ë¹„êµ")
print("="*80)

print(f"\n{'ëª¨ë¸':<35} {'Accuracy':>12} {'Macro F1':>12} {'ìƒí™œ F1':>12}")
print("-"*75)
print(f"{'Baseline (Original)':<35} {0.4913:>12.4f} {0.4344:>12.4f} {0.0802:>12.4f}")
print(f"{'XGBoost (ì¼ë°˜)':<35} {acc_xgb_normal:>12.4f} {f1_xgb_normal:>12.4f} {cat_f1_xgb_normal[1]:>12.4f}")
print(f"{'XGBoost (Focal Loss Î³=2)':<35} {acc_xgb_focal:>12.4f} {f1_xgb_focal:>12.4f} {cat_f1_xgb_focal[1]:>12.4f}")
print(f"{'LightGBM (Weight 2.5x)':<35} {acc_lgb:>12.4f} {f1_lgb:>12.4f} {cat_f1_lgb[1]:>12.4f}")
print("-"*75)

# Focal Loss íš¨ê³¼
focal_vs_normal_acc = (acc_xgb_focal - acc_xgb_normal) * 100
focal_vs_normal_f1 = (cat_f1_xgb_focal[1] - cat_f1_xgb_normal[1]) * 100

print(f"\nğŸ“Š Focal Loss íš¨ê³¼:")
print(f"  Accuracy:  {focal_vs_normal_acc:+.2f}%p")
print(f"  ìƒí™œ F1:   {focal_vs_normal_f1:+.2f}%p")

if focal_vs_normal_f1 > 0:
    print(f"  âœ… Focal Lossê°€ ìƒí™œ ì¹´í…Œê³ ë¦¬ ê°œì„ !")
else:
    print(f"  âš ï¸ Focal Loss íš¨ê³¼ ì œí•œì ")

print("\n" + "="*80)
print("âœ… STEP 4 ì™„ë£Œ!")
print("="*80)
print(f"\nğŸ“¦ ë‹¤ìŒ ë‹¨ê³„: Stacking Ensemble")
print("="*80)
