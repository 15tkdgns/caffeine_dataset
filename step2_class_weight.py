"""
STEP 2: Class Weight ì ìš© + ì „ëµì  ì–¸ë”ìƒ˜í”Œë§
- ìƒí™œ ì¹´í…Œê³ ë¦¬ ê°€ì¤‘ì¹˜ ì¦ê°€
- ì‹ë£Œí’ˆ, ì‡¼í•‘, ì™¸ì‹ ì–¸ë”ìƒ˜í”Œë§ (0.7ë°°)
- LightGBM, XGBoost ë¹„êµ
"""

import numpy as np
import pandas as pd
import lightgbm as lgb
import xgboost as xgb
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.utils.class_weight import compute_class_weight
import joblib
import json
from datetime import datetime
import time

print("="*80)
print("âš–ï¸ STEP 2: Class Weight + ì „ëµì  ì–¸ë”ìƒ˜í”Œë§")
print("="*80)

# ============================================================
# 1. ë°ì´í„° ë¡œë“œ
# ============================================================
print("\n[1/6] ë°ì´í„° ë¡œë“œ")

X_train = np.load('02_data/02_augmented/X_train_smote.npy')
y_train = np.load('02_data/02_augmented/y_train_smote.npy')
X_test = np.load('02_data/02_augmented/X_test.npy')
y_test = np.load('02_data/02_augmented/y_test.npy')

print(f"  ì›ë³¸ í•™ìŠµ ë°ì´í„°: {len(X_train):,}ê±´")
print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test):,}ê±´")

category_names = ['êµí†µ', 'ìƒí™œ', 'ì‡¼í•‘', 'ì‹ë£Œí’ˆ', 'ì™¸ì‹', 'ì£¼ìœ ']

# ============================================================
# 2. ì „ëµì  ì–¸ë”ìƒ˜í”Œë§
# ============================================================
print("\n[2/6] ì „ëµì  ì–¸ë”ìƒ˜í”Œë§")

# ë¶„ì„ ê²°ê³¼: ìƒí™œê³¼ ê°€ì¥ í—·ê°ˆë¦¬ëŠ” í´ë˜ìŠ¤
# 1. ì‹ë£Œí’ˆ (43,581ê±´ í˜¼ë™)
# 2. ì‡¼í•‘ (29,883ê±´ í˜¼ë™)  
# 3. ì™¸ì‹ (14,652ê±´ í˜¼ë™)

# í´ë˜ìŠ¤ë³„ ìƒ˜í”Œë§ ë¹„ìœ¨
sampling_ratios = {
    0: 1.0,   # êµí†µ - ìœ ì§€
    1: 1.0,   # ìƒí™œ - ìœ ì§€ (ì¦ê°•í•˜ê³  ì‹¶ì§€ë§Œ SMOTE ì´ë¯¸ ì ìš©ë¨)
    2: 0.7,   # ì‡¼í•‘ - 70%ë¡œ ê°ì†Œ (ìƒí™œê³¼ í—·ê°ˆë¦¼)
    3: 0.7,   # ì‹ë£Œí’ˆ - 70%ë¡œ ê°ì†Œ (ìƒí™œê³¼ ê°€ì¥ í—·ê°ˆë¦¼)
    4: 0.7,   # ì™¸ì‹ - 70%ë¡œ ê°ì†Œ (ìƒí™œê³¼ í—·ê°ˆë¦¼)
    5: 1.0    # ì£¼ìœ  - ìœ ì§€
}

# ìƒ˜í”Œë§ ìˆ˜í–‰
indices_to_keep = []
for class_id in range(6):
    class_mask = (y_train == class_id)
    class_indices = np.where(class_mask)[0]
    
    n_samples = len(class_indices)
    n_keep = int(n_samples * sampling_ratios[class_id])
    
    # ëœë¤ ìƒ˜í”Œë§
    np.random.seed(42)
    kept_indices = np.random.choice(class_indices, n_keep, replace=False)
    indices_to_keep.extend(kept_indices)
    
    print(f"  {category_names[class_id]:6s}: {n_samples:,}ê±´ â†’ {n_keep:,}ê±´ ({sampling_ratios[class_id]*100:.0f}%)")

indices_to_keep = np.array(indices_to_keep)
X_train_sampled = X_train[indices_to_keep]
y_train_sampled = y_train[indices_to_keep]

print(f"\n  ì´ í•™ìŠµ ë°ì´í„°: {len(X_train):,}ê±´ â†’ {len(X_train_sampled):,}ê±´")

# ============================================================
# 3. Class Weight ê³„ì‚°
# ============================================================
print("\n[3/6] Class Weight ê³„ì‚°")

# ìë™ ê³„ì‚° (balanced)
class_weights_auto = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train_sampled),
    y=y_train_sampled
)

print(f"\n  Balanced Class Weights:")
for i, (cat, weight) in enumerate(zip(category_names, class_weights_auto)):
    print(f"     {cat:6s}: {weight:.4f}")

# ìˆ˜ë™ ì¡°ì •: ìƒí™œ ì¹´í…Œê³ ë¦¬ 3ë°° ë¶€ìŠ¤íŒ…
class_weights_manual = class_weights_auto.copy()
class_weights_manual[1] *= 3.0  # ìƒí™œ ì¹´í…Œê³ ë¦¬

print(f"\n  Manual (ìƒí™œ 3ë°° ë¶€ìŠ¤íŒ…):")
for i, (cat, weight) in enumerate(zip(category_names, class_weights_manual)):
    print(f"     {cat:6s}: {weight:.4f}")

# Sample weight ìƒì„±
sample_weights = np.array([class_weights_manual[y] for y in y_train_sampled])

# ============================================================
# 4. LightGBM with Class Weight
# ============================================================
print("\n[4/6] LightGBM with Class Weight")

print("  í•™ìŠµ ì‹œì‘...")
start = time.time()

lgb_model = lgb.LGBMClassifier(
    n_estimators=300,
    max_depth=10,
    learning_rate=0.1,
    num_leaves=128,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
    verbose=-1
)

lgb_model.fit(X_train_sampled, y_train_sampled, sample_weight=sample_weights)
train_time_lgb = time.time() - start

# ì˜ˆì¸¡
y_pred_lgb = lgb_model.predict(X_test)
y_proba_lgb = lgb_model.predict_proba(X_test)

# í‰ê°€
acc_lgb = accuracy_score(y_test, y_pred_lgb)
macro_f1_lgb = f1_score(y_test, y_pred_lgb, average='macro')
weighted_f1_lgb = f1_score(y_test, y_pred_lgb, average='weighted')
category_f1_lgb = f1_score(y_test, y_pred_lgb, average=None)

print(f"  âœ… í•™ìŠµ ì™„ë£Œ: {train_time_lgb:.2f}ì´ˆ")
print(f"\n  ğŸ“Š ì„±ëŠ¥:")
print(f"     Accuracy:    {acc_lgb:.4f} ({acc_lgb*100:.2f}%)")
print(f"     Macro F1:    {macro_f1_lgb:.4f} ({macro_f1_lgb*100:.2f}%)")
print(f"     Weighted F1: {weighted_f1_lgb:.4f}")

print(f"\n  ì¹´í…Œê³ ë¦¬ë³„ F1:")
for cat, f1 in zip(category_names, category_f1_lgb):
    emoji = "â­" if f1 > 0.5 else "âœ…" if f1 > 0.3 else "âš ï¸"
    improvement = ""
    if cat == "ìƒí™œ":
        baseline_f1 = 0.0802
        imp = (f1 - baseline_f1) * 100
        improvement = f" ({imp:+.2f}%p from baseline)"
    print(f"     {emoji} {cat:6s}: {f1:.4f}{improvement}")

# ============================================================
# 5. XGBoost with Class Weight
# ============================================================
print("\n[5/6] XGBoost with Class Weight")

print("  í•™ìŠµ ì‹œì‘...")
start = time.time()

xgb_model = xgb.XGBClassifier(
    device='cuda',
    tree_method='hist',
    n_estimators=300,
    max_depth=10,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)

xgb_model.fit(X_train_sampled, y_train_sampled, sample_weight=sample_weights)
train_time_xgb = time.time() - start

# ì˜ˆì¸¡
y_pred_xgb = xgb_model.predict(X_test)
y_proba_xgb = xgb_model.predict_proba(X_test)

# í‰ê°€
acc_xgb = accuracy_score(y_test, y_pred_xgb)
macro_f1_xgb = f1_score(y_test, y_pred_xgb, average='macro')
weighted_f1_xgb = f1_score(y_test, y_pred_xgb, average='weighted')
category_f1_xgb = f1_score(y_test, y_pred_xgb, average=None)

print(f"  âœ… í•™ìŠµ ì™„ë£Œ: {train_time_xgb:.2f}ì´ˆ")
print(f"\n  ğŸ“Š ì„±ëŠ¥:")
print(f"     Accuracy:    {acc_xgb:.4f} ({acc_xgb*100:.2f}%)")
print(f"     Macro F1:    {macro_f1_xgb:.4f} ({macro_f1_xgb*100:.2f}%)")
print(f"     Weighted F1: {weighted_f1_xgb:.4f}")

print(f"\n  ì¹´í…Œê³ ë¦¬ë³„ F1:")
for cat, f1 in zip(category_names, category_f1_xgb):
    emoji = "â­" if f1 > 0.5 else "âœ…" if f1 > 0.3 else "âš ï¸"
    improvement = ""
    if cat == "ìƒí™œ":
        baseline_f1 = 0.0802
        imp = (f1 - baseline_f1) * 100
        improvement = f" ({imp:+.2f}%p from baseline)"
    print(f"     {emoji} {cat:6s}: {f1:.4f}{improvement}")

# ============================================================
# 6. ê²°ê³¼ ì €ì¥ ë° ë¹„êµ
# ============================================================
print("\n[6/6] ê²°ê³¼ ì €ì¥")

import os
os.makedirs('04_logs/step2_class_weight', exist_ok=True)

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
if category_f1_lgb[1] > category_f1_xgb[1]:  # ìƒí™œ F1 ê¸°ì¤€
    best_model = lgb_model
    best_name = 'LightGBM'
    best_metrics = {
        'accuracy': acc_lgb,
        'macro_f1': macro_f1_lgb,
        'category_f1': category_f1_lgb
    }
else:
    best_model = xgb_model
    best_name = 'XGBoost'
    best_metrics = {
        'accuracy': acc_xgb,
        'macro_f1': macro_f1_xgb,
        'category_f1': category_f1_xgb
    }

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# ëª¨ë¸ ì €ì¥
model_path = f'04_logs/step2_class_weight/{best_name.lower()}_weighted_{timestamp}.joblib'
joblib.dump(best_model, model_path)
print(f"  âœ… ìµœê³  ëª¨ë¸ ì €ì¥: {model_path}")

# ë©”íƒ€ë°ì´í„°
metadata = {
    'experiment': 'STEP 2: Class Weight + Undersampling',
    'strategy': {
        'undersampling': sampling_ratios,
        'class_weights': {cat: float(w) for cat, w in zip(category_names, class_weights_manual)},
        'living_boost_factor': 3.0
    },
    'results': {
        'lightgbm': {
            'accuracy': float(acc_lgb),
            'macro_f1': float(macro_f1_lgb),
            'category_f1': {cat: float(f1) for cat, f1 in zip(category_names, category_f1_lgb)},
            'train_time': float(train_time_lgb)
        },
        'xgboost': {
            'accuracy': float(acc_xgb),
            'macro_f1': float(macro_f1_xgb),
            'category_f1': {cat: float(f1) for cat, f1 in zip(category_names, category_f1_xgb)},
            'train_time': float(train_time_xgb)
        },
        'best_model': best_name
    },
    'comparison': {
        'baseline_acc': 0.4913,
        'baseline_macro_f1': 0.4344,
        'baseline_living_f1': 0.0802
    }
}

metadata_path = f'04_logs/step2_class_weight/metadata_{timestamp}.json'
with open(metadata_path, 'w', encoding='utf-8') as f:
    json.dump(metadata, f, indent=2, ensure_ascii=False)
print(f"  âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")

# ============================================================
# 7. ìµœì¢… ë¹„êµ
# ============================================================
print("\n" + "="*80)
print("ğŸ† STEP 2 ê²°ê³¼ ë¹„êµ")
print("="*80)

baseline = {'acc': 0.4913, 'f1': 0.4344, 'living_f1': 0.0802}

print(f"\n{'ëª¨ë¸':<40} {'Accuracy':>12} {'Macro F1':>12} {'ìƒí™œ F1':>12}")
print("-"*80)
print(f"{'Baseline (ì›ë³¸)':<40} {baseline['acc']:>12.4f} {baseline['f1']:>12.4f} {baseline['living_f1']:>12.4f}")
print(f"{'LightGBM (Weight+Undersample)':<40} {acc_lgb:>12.4f} {macro_f1_lgb:>12.4f} {category_f1_lgb[1]:>12.4f}")
print(f"{'XGBoost (Weight+Undersample)':<40} {acc_xgb:>12.4f} {macro_f1_xgb:>12.4f} {category_f1_xgb[1]:>12.4f}")
print("-"*80)

print(f"\nğŸ“Š ë³€í™”ëŸ‰:")
print(f"  LightGBM:")
print(f"    Accuracy:  {(acc_lgb - baseline['acc'])*100:+.2f}%p")
print(f"    Macro F1:  {(macro_f1_lgb - baseline['f1'])*100:+.2f}%p")
print(f"    ìƒí™œ F1:   {(category_f1_lgb[1] - baseline['living_f1'])*100:+.2f}%p â­")

print(f"\n  XGBoost:")
print(f"    Accuracy:  {(acc_xgb - baseline['acc'])*100:+.2f}%p")
print(f"    Macro F1:  {(macro_f1_xgb - baseline['f1'])*100:+.2f}%p")
print(f"    ìƒí™œ F1:   {(category_f1_xgb[1] - baseline['living_f1'])*100:+.2f}%p â­")

# í‰ê°€
living_improved = category_f1_lgb[1] > baseline['living_f1'] or category_f1_xgb[1] > baseline['living_f1']
acc_maintained = acc_lgb > 0.45 or acc_xgb > 0.45

print(f"\nğŸ¯ í‰ê°€:")
if living_improved and acc_maintained:
    print(f"  âœ…âœ… ì„±ê³µ! ìƒí™œ F1 ê°œì„  + Accuracy ìœ ì§€")
elif living_improved:
    print(f"  âœ… ìƒí™œ F1 ê°œì„ ë¨, í•˜ì§€ë§Œ Accuracy í•˜ë½")
else:
    print(f"  âš ï¸ ì¶”ê°€ ì¡°ì • í•„ìš”")

print("\n" + "="*80)
print("âœ… STEP 2 ì™„ë£Œ!")
print("="*80)
print(f"\nğŸ“¦ ë‹¤ìŒ ë‹¨ê³„:")
print(f"   STEP 3: SMOTE/ADASYN (ìƒí™œ ì¹´í…Œê³ ë¦¬ë§Œ ì¦ê°•)")
print(f"   STEP 4: Focal Loss ë„ì…")
print("="*80)
