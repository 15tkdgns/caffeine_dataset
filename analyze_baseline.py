"""
STEP 1: Baseline ë¶„ì„
- Confusion Matrix
- í´ë˜ìŠ¤ë³„ í˜¼ë™ íŒ¨í„´
- Feature Importance
- ìƒí™œ ì¹´í…Œê³ ë¦¬ ì˜¤ë¶„ë¥˜ ë¶„ì„
"""

import numpy as np
import pandas as pd
import joblib
from sklearn.metrics import confusion_matrix, classification_report
import json

print("="*80)
print("ğŸ“Š STEP 1: Baseline ëª¨ë¸ ìƒì„¸ ë¶„ì„")
print("="*80)

# ============================================================
# 1. ë°ì´í„° ë° ëª¨ë¸ ë¡œë“œ
# ============================================================
print("\n[1/5] ë°ì´í„° ë° ëª¨ë¸ ë¡œë“œ")

X_test = np.load('02_data/02_augmented/X_test.npy')
y_test = np.load('02_data/02_augmented/y_test.npy')

# Baseline LightGBM ë¡œë“œ
model = joblib.load('03_models/production_models/lightgbm_cuda_production_20251205_162340.joblib')
print(f"  âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test):,}ê±´")

# ì˜ˆì¸¡
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)

category_names = ['êµí†µ', 'ìƒí™œ', 'ì‡¼í•‘', 'ì‹ë£Œí’ˆ', 'ì™¸ì‹', 'ì£¼ìœ ']

# ============================================================
# 2. Confusion Matrix
# ============================================================
print("\n[2/5] Confusion Matrix ë¶„ì„")

cm = confusion_matrix(y_test, y_pred)

# ì •ê·œí™”ëœ confusion matrix (í–‰ ê¸°ì¤€)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

print(f"\n  Confusion Matrix (ì ˆëŒ€ê°’):")
cm_df = pd.DataFrame(cm, index=category_names, columns=category_names)
print(cm_df)

print(f"\n  Confusion Matrix (ì •ê·œí™” - í–‰ ê¸°ì¤€):")
cm_norm_df = pd.DataFrame(cm_normalized, index=category_names, columns=category_names)
print(cm_norm_df.round(3))

# ìƒí™œ ì¹´í…Œê³ ë¦¬ ë¶„ì„
ìƒí™œ_idx = 1
print(f"\n  ğŸ” ìƒí™œ ì¹´í…Œê³ ë¦¬ ìƒì„¸ ë¶„ì„:")
print(f"     ì •ë‹µ ìƒí™œ ê±´ìˆ˜: {cm[ìƒí™œ_idx].sum():,}ê±´")
print(f"     ì •í™•íˆ ë§ì¶˜ ê±´ìˆ˜: {cm[ìƒí™œ_idx, ìƒí™œ_idx]:,}ê±´")
print(f"     ì •í™•ë„: {cm[ìƒí™œ_idx, ìƒí™œ_idx] / cm[ìƒí™œ_idx].sum() * 100:.2f}%")

# ìƒí™œì´ ì˜¤ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬
print(f"\n  ìƒí™œ â†’ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ ì˜¤ë¶„ë¥˜:")
for i, cat in enumerate(category_names):
    if i != ìƒí™œ_idx:
        count = cm[ìƒí™œ_idx, i]
        ratio = count / cm[ìƒí™œ_idx].sum() * 100
        print(f"     â†’ {cat}: {count:,}ê±´ ({ratio:.2f}%)")

# ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ â†’ ìƒí™œ ì˜¤ë¶„ë¥˜
print(f"\n  ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ â†’ ìƒí™œ ì˜¤ë¶„ë¥˜:")
for i, cat in enumerate(category_names):
    if i != ìƒí™œ_idx:
        count = cm[i, ìƒí™œ_idx]
        ratio = count / cm[i].sum() * 100
        if count > 0:
            print(f"     {cat} â†’ ìƒí™œ: {count:,}ê±´ ({ratio:.2f}%)")

# ============================================================
# 3. í´ë˜ìŠ¤ ë¶„í¬
# ============================================================
print("\n[3/5] í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„")

unique, counts = np.unique(y_test, return_counts=True)
total = len(y_test)

print(f"\n  í…ŒìŠ¤íŠ¸ ë°ì´í„° í´ë˜ìŠ¤ ë¶„í¬:")
for cat_id, cat_name, count in zip(unique, category_names, counts):
    ratio = count / total * 100
    print(f"     {cat_name:6s}: {count:,}ê±´ ({ratio:.2f}%)")

# ê°€ì¥ ë¶ˆê· í˜•í•œ í´ë˜ìŠ¤
max_count = counts.max()
min_count = counts.min()
imbalance_ratio = max_count / min_count
print(f"\n  ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f}:1 (ìµœëŒ€/ìµœì†Œ)")

# ============================================================
# 4. Feature Importance
# ============================================================
print("\n[4/5] Feature Importance ë¶„ì„")

# LightGBM feature importance
importance = model.feature_importances_

# ìƒìœ„ 20ê°œ í”¼ì²˜
top_k = 20
top_indices = np.argsort(importance)[::-1][:top_k]

print(f"\n  Top {top_k} ì¤‘ìš” í”¼ì²˜:")
for rank, idx in enumerate(top_indices, 1):
    print(f"     {rank:2d}. Feature {idx:2d}: {importance[idx]:.4f}")

# Feature importance ì €ì¥
importance_data = {
    'feature_importance': {
        f'feature_{i}': float(imp) 
        for i, imp in enumerate(importance)
    },
    'top_20_features': [int(idx) for idx in top_indices]
}

# ============================================================
# 5. ìƒí™œ ì¹´í…Œê³ ë¦¬ ì˜¤ë¶„ë¥˜ ì‹¬ì¸µ ë¶„ì„
# ============================================================
print("\n[5/5] ìƒí™œ ì¹´í…Œê³ ë¦¬ ì˜¤ë¶„ë¥˜ ì‹¬ì¸µ ë¶„ì„")

# ìƒí™œë¡œ ì˜ˆì¸¡ëœ ìƒ˜í”Œ ë¶„ì„
ìƒí™œ_mask = (y_test == ìƒí™œ_idx)
ìƒí™œ_pred_mask = (y_pred == ìƒí™œ_idx)

# True Positive (ì •ë‹µ)
tp_mask = ìƒí™œ_mask & ìƒí™œ_pred_mask
tp_count = tp_mask.sum()

# False Negative (ìƒí™œì¸ë° ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ ì˜ˆì¸¡)
fn_mask = ìƒí™œ_mask & ~ìƒí™œ_pred_mask
fn_count = fn_mask.sum()

# False Positive (ìƒí™œì´ ì•„ë‹Œë° ìƒí™œë¡œ ì˜ˆì¸¡)
fp_mask = ~ìƒí™œ_mask & ìƒí™œ_pred_mask
fp_count = fp_mask.sum()

print(f"\n  ìƒí™œ ì¹´í…Œê³ ë¦¬ ì˜ˆì¸¡ ê²°ê³¼:")
print(f"     True Positive:  {tp_count:,}ê±´")
print(f"     False Negative: {fn_count:,}ê±´ (ìƒí™œì„ ëª» ë§ì¶¤)")
print(f"     False Positive: {fp_count:,}ê±´ (ë‹¤ë¥¸ ê±¸ ìƒí™œë¡œ ì˜¤íŒ)")

# FN ë¶„ì„ (ìƒí™œì„ ëª» ë§ì¶˜ ê²½ìš°)
if fn_count > 0:
    fn_predictions = y_pred[fn_mask]
    print(f"\n  ìƒí™œì„ ëª» ë§ì¶˜ ê²½ìš°, ì–´ë–¤ ì¹´í…Œê³ ë¦¬ë¡œ ì˜ˆì¸¡í–ˆëŠ”ì§€:")
    fn_unique, fn_counts = np.unique(fn_predictions, return_counts=True)
    for pred_id, count in sorted(zip(fn_unique, fn_counts), key=lambda x: -x[1]):
        ratio = count / fn_count * 100
        print(f"     â†’ {category_names[pred_id]}: {count:,}ê±´ ({ratio:.2f}%)")

# FP ë¶„ì„ (ìƒí™œë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ê²½ìš°)
if fp_count > 0:
    fp_true_labels = y_test[fp_mask]
    print(f"\n  ìƒí™œë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ê²½ìš°, ì‹¤ì œë¡œëŠ”:")
    fp_unique, fp_counts = np.unique(fp_true_labels, return_counts=True)
    for true_id, count in sorted(zip(fp_unique, fp_counts), key=lambda x: -x[1]):
        ratio = count / fp_count * 100
        print(f"     ì‹¤ì œ {category_names[true_id]}: {count:,}ê±´ ({ratio:.2f}%)")

# ============================================================
# 6. ê²°ê³¼ ì €ì¥
# ============================================================
print("\n[6/6] ê²°ê³¼ ì €ì¥")

import os
os.makedirs('04_logs/analysis', exist_ok=True)

# Confusion Matrix ì €ì¥
analysis_results = {
    'confusion_matrix': {
        'absolute': cm.tolist(),
        'normalized': cm_normalized.tolist()
    },
    'class_distribution': {
        cat: int(count) for cat, count in zip(category_names, counts)
    },
    'living_category_analysis': {
        'true_positive': int(tp_count),
        'false_negative': int(fn_count),
        'false_positive': int(fp_count),
        'accuracy': float(tp_count / (tp_count + fn_count)) if (tp_count + fn_count) > 0 else 0,
        'precision': float(tp_count / (tp_count + fp_count)) if (tp_count + fp_count) > 0 else 0,
        'recall': float(tp_count / (tp_count + fn_count)) if (tp_count + fn_count) > 0 else 0
    },
    'most_confused_with_living': {},
    'feature_importance': importance_data
}

# ìƒí™œê³¼ ê°€ì¥ í—·ê°ˆë¦¬ëŠ” í´ë˜ìŠ¤ (ì–‘ë°©í–¥)
for i, cat in enumerate(category_names):
    if i != ìƒí™œ_idx:
        # ìƒí™œ â†’ cat
        ìƒí™œ_to_cat = int(cm[ìƒí™œ_idx, i])
        # cat â†’ ìƒí™œ
        cat_to_ìƒí™œ = int(cm[i, ìƒí™œ_idx])
        
        if ìƒí™œ_to_cat > 0 or cat_to_ìƒí™œ > 0:
            analysis_results['most_confused_with_living'][cat] = {
                'living_to_category': ìƒí™œ_to_cat,
                'category_to_living': cat_to_ìƒí™œ,
                'total_confusion': ìƒí™œ_to_cat + cat_to_ìƒí™œ
            }

# JSON ì €ì¥
with open('04_logs/analysis/baseline_analysis.json', 'w', encoding='utf-8') as f:
    json.dump(analysis_results, f, indent=2, ensure_ascii=False)

print(f"  âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥: 04_logs/analysis/baseline_analysis.json")

# ============================================================
# 7. í•µì‹¬ ë°œê²¬ ìš”ì•½
# ============================================================
print("\n" + "="*80)
print("ğŸ¯ í•µì‹¬ ë°œê²¬ ì‚¬í•­")
print("="*80)

# ìƒí™œê³¼ ê°€ì¥ í—·ê°ˆë¦¬ëŠ” Top 3 í´ë˜ìŠ¤
confused_sorted = sorted(
    analysis_results['most_confused_with_living'].items(),
    key=lambda x: x[1]['total_confusion'],
    reverse=True
)[:3]

print(f"\nâœ… ìƒí™œ ì¹´í…Œê³ ë¦¬ì™€ ê°€ì¥ í—·ê°ˆë¦¬ëŠ” í´ë˜ìŠ¤ (Top 3):")
for rank, (cat, conf) in enumerate(confused_sorted, 1):
    total = conf['total_confusion']
    ìƒí™œ_to = conf['living_to_category']
    to_ìƒí™œ = conf['category_to_living']
    print(f"   {rank}. {cat}: ì´ {total:,}ê±´ í˜¼ë™")
    print(f"      - ìƒí™œ â†’ {cat}: {ìƒí™œ_to:,}ê±´")
    print(f"      - {cat} â†’ ìƒí™œ: {to_ìƒí™œ:,}ê±´")

print(f"\nâœ… Feature Importance Top 5:")
for i in range(min(5, len(top_indices))):
    idx = top_indices[i]
    print(f"   {i+1}. Feature {idx}: {importance[idx]:.4f}")

print(f"\nâœ… í´ë˜ìŠ¤ ë¶ˆê· í˜•:")
print(f"   ìµœëŒ€: {category_names[counts.argmax()]} ({counts.max():,}ê±´)")
print(f"   ìµœì†Œ: {category_names[counts.argmin()]} ({counts.min():,}ê±´)")
print(f"   ë¹„ìœ¨: {imbalance_ratio:.2f}:1")

print("\n" + "="*80)
print("âœ… Baseline ë¶„ì„ ì™„ë£Œ!")
print("="*80)
print(f"\nğŸ“¦ ë‹¤ìŒ ë‹¨ê³„:")
print(f"   STEP 2: Class Weight ì ìš©")
print(f"   STEP 3: SMOTE/ADASYN (ìƒí™œ ì¹´í…Œê³ ë¦¬ ì¦ê°•)")
print(f"   STEP 4: Focal Loss ë„ì…")
print("="*80)
