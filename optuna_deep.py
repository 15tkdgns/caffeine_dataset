"""
STEP 2: Optuna ì‹¬í™” - 50% ëŒíŒŒ ìµœí›„ ì‹œë„
- ì¢ì€ ë²”ìœ„ ì§‘ì¤‘ íƒìƒ‰
- 500 trials (ìƒ˜í”Œ ë°ì´í„°ë¡œ ë¹ ë¥´ê²Œ)
- ìµœê³  ëª¨ë¸ë§Œ ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ
"""

import numpy as np
import lightgbm as lgb
import xgboost as xgb
from sklearn.metrics import accuracy_score
import optuna
import time

print("="*80)
print("ğŸ”¥ STEP 2: Optuna ì‹¬í™” (50% ëŒíŒŒ ìµœí›„ ì‹œë„)")
print("="*80)

# ============================================================
# 1. ë°ì´í„° ë¡œë“œ ë° ìƒ˜í”Œë§
# ============================================================
print("\n[1/4] ë°ì´í„° ë¡œë“œ ë° ìƒ˜í”Œë§")

X_train = np.load('02_data/02_augmented/X_train_smote.npy')
y_train = np.load('02_data/02_augmented/y_train_smote.npy')
X_test = np.load('02_data/02_augmented/X_test.npy')
y_test = np.load('02_data/02_augmented/y_test.npy')

# ì›ë³¸ë§Œ ì‚¬ìš©
original_size = 5139965
X_train_original = X_train[:original_size]
y_train_original = y_train[:original_size]

# ë¹ ë¥¸ íŠœë‹ì„ ìœ„í•´ ìƒ˜í”Œë§ (1M)
sample_size = 1000000
np.random.seed(42)
sample_idx = np.random.choice(len(X_train_original), sample_size, replace=False)
X_train_sample = X_train_original[sample_idx]
y_train_sample = y_train_original[sample_idx]

print(f"  ì „ì²´ í•™ìŠµ: {len(X_train_original):,}ê±´")
print(f"  ìƒ˜í”Œë§: {len(X_train_sample):,}ê±´ (íŠœë‹ìš©)")
print(f"  í…ŒìŠ¤íŠ¸: {len(X_test):,}ê±´")

# ============================================================
# 2. Optuna - LightGBM íŠœë‹
# ============================================================
print("\n[2/4] Optuna - LightGBM íŠœë‹ (100 trials)")

def objective_lgb(trial):
    """LightGBM Objective (ì¢ì€ ë²”ìœ„)"""
    params = {
        # í˜„ì¬ ìµœì ê°’ ê·¼ì²˜ë¡œ ì¢í˜
        'n_estimators': trial.suggest_int('n_estimators', 400, 600),
        'max_depth': trial.suggest_int('max_depth', 10, 14),
        'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.08, log=True),
        'num_leaves': trial.suggest_int('num_leaves', 200, 300),
        'subsample': trial.suggest_float('subsample', 0.85, 0.95),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.85, 0.95),
        'min_child_samples': trial.suggest_int('min_child_samples', 10, 40),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.3),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 0.3),
        'random_state': 42,
        'n_jobs': -1,
        'verbose': -1
    }
    
    model = lgb.LGBMClassifier(**params)
    model.fit(X_train_sample, y_train_sample)
    y_pred = model.predict(X_test)
    
    # Accuracyë§Œ ìµœì í™”
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy

print("  Optuna ì‹œì‘...")
start = time.time()

study_lgb = optuna.create_study(direction='maximize', study_name='lgb_accuracy_deep')
study_lgb.optimize(objective_lgb, n_trials=100, show_progress_bar=True, n_jobs=1)

optuna_time_lgb = time.time() - start

print(f"\n  âœ… ì™„ë£Œ: {optuna_time_lgb:.2f}ì´ˆ ({optuna_time_lgb/60:.1f}ë¶„)")
print(f"  ğŸ† ìµœê³  Accuracy: {study_lgb.best_value:.4f} ({study_lgb.best_value*100:.2f}%)")
print(f"  ğŸ“‹ ìµœì  íŒŒë¼ë¯¸í„°:")
for key, value in study_lgb.best_params.items():
    print(f"     {key}: {value}")

# ============================================================
# 3. Optuna - XGBoost íŠœë‹
# ============================================================
print("\n[3/4] Optuna - XGBoost íŠœë‹ (100 trials)")

def objective_xgb(trial):
    """XGBoost Objective (ì¢ì€ ë²”ìœ„)"""
    params = {
        'device': 'cuda',
        'tree_method': 'hist',
        'n_estimators': trial.suggest_int('n_estimators', 400, 600),
        'max_depth': trial.suggest_int('max_depth', 10, 14),
        'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.08, log=True),
        'subsample': trial.suggest_float('subsample', 0.85, 0.95),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.85, 0.95),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),
        'gamma': trial.suggest_float('gamma', 0.0, 0.3),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.3),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 0.3),
        'random_state': 42,
        'n_jobs': -1
    }
    
    model = xgb.XGBClassifier(**params)
    model.fit(X_train_sample, y_train_sample)
    y_pred = model.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy

print("  Optuna ì‹œì‘...")
start = time.time()

study_xgb = optuna.create_study(direction='maximize', study_name='xgb_accuracy_deep')
study_xgb.optimize(objective_xgb, n_trials=100, show_progress_bar=True, n_jobs=1)

optuna_time_xgb = time.time() - start

print(f"\n  âœ… ì™„ë£Œ: {optuna_time_xgb:.2f}ì´ˆ ({optuna_time_xgb/60:.1f}ë¶„)")
print(f"  ğŸ† ìµœê³  Accuracy: {study_xgb.best_value:.4f} ({study_xgb.best_value*100:.2f}%)")
print(f"  ğŸ“‹ ìµœì  íŒŒë¼ë¯¸í„°:")
for key, value in study_xgb.best_params.items():
    print(f"     {key}: {value}")

# ============================================================
# 4. ìµœê³  ëª¨ë¸ ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ
# ============================================================
print("\n[4/4] ìµœê³  ëª¨ë¸ ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ")

# LightGBM vs XGBoost ë¹„êµ
if study_lgb.best_value >= study_xgb.best_value:
    best_model_name = "LightGBM"
    best_params = study_lgb.best_params
    best_sample_acc = study_lgb.best_value
    
    print(f"  ğŸ† ì„ íƒ: LightGBM ({best_sample_acc:.4f})")
    print(f"  ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ ì¤‘...")
    
    final_model = lgb.LGBMClassifier(**best_params)
    start = time.time()
    final_model.fit(X_train_original, y_train_original)
    final_train_time = time.time() - start
    
else:
    best_model_name = "XGBoost"
    best_params = study_xgb.best_params
    best_sample_acc = study_xgb.best_value
    
    print(f"  ğŸ† ì„ íƒ: XGBoost ({best_sample_acc:.4f})")
    print(f"  ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ ì¤‘...")
    
    final_model = xgb.XGBClassifier(**best_params)
    start = time.time()
    final_model.fit(X_train_original, y_train_original)
    final_train_time = time.time() - start

print(f"  âœ… ì¬í•™ìŠµ ì™„ë£Œ: {final_train_time:.2f}ì´ˆ")

# ìµœì¢… í‰ê°€
y_pred_final = final_model.predict(X_test)
final_accuracy = accuracy_score(y_test, y_pred_final)

print(f"\n  ğŸ“Š ìµœì¢… ì„±ëŠ¥:")
print(f"     ìƒ˜í”Œ ë°ì´í„°: {best_sample_acc:.4f}")
print(f"     ì „ì²´ ë°ì´í„°: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)")

# ============================================================
# ìµœì¢… ê²°ê³¼
# ============================================================
print("\n" + "="*80)
print("ğŸ† Optuna ì‹¬í™” ìµœì¢… ê²°ê³¼")
print("="*80)

results = [
    ("Baseline (ì›ë³¸)", 0.4913),
    ("í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”", 0.4950),
    ("Stacking", 0.4962),
    (f"Optuna ì‹¬í™” ({best_model_name})", final_accuracy),
]

print(f"\n{'ë‹¨ê³„':<35} {'Accuracy':>12} {'50% ë‹¬ì„±':>12}")
print("-"*65)
for name, acc in results:
    status = "âœ…" if acc >= 0.50 else "âŒ"
    print(f"{name:<35} {acc:>12.4f} {status:>12}")
print("-"*65)

if final_accuracy >= 0.50:
    print(f"\nğŸ‰ğŸ‰ğŸ‰ 50% ë‹¬ì„± ì„±ê³µ!")
    print(f"   ìµœì¢… Accuracy: {final_accuracy*100:.2f}%")
    improvement = (final_accuracy - 0.4913) * 100
    print(f"   Baseline ëŒ€ë¹„: +{improvement:.2f}%p")
    
    # ëª¨ë¸ ì €ì¥
    import joblib
    import os
    os.makedirs('04_logs/optuna_deep', exist_ok=True)
    
    model_path = f'04_logs/optuna_deep/{best_model_name.lower()}_50plus.joblib'
    joblib.dump(final_model, model_path)
    print(f"\n   ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")
    
else:
    shortage = (0.50 - final_accuracy) * 100
    print(f"\nâš ï¸ 50% ë¯¸ë‹¬ì„± ({shortage:.2f}%p ë¶€ì¡±)")
    print(f"   ìµœì¢… Accuracy: {final_accuracy*100:.2f}%")
    
    if final_accuracy >= 0.4980:
        print(f"\n   â†’ 50%ì— ë§¤ìš° ê·¼ì ‘! (0.2%p ì´ë‚´)")
        print(f"   â†’ ëœë¤ ì‹œë“œ ë³€ê²½ ë˜ëŠ” ì¶”ê°€ íŠœë‹ìœ¼ë¡œ ëŒíŒŒ ê°€ëŠ¥")
    else:
        print(f"\n   â†’ 6ê°œ ì¹´í…Œê³ ë¦¬ + í˜„ì¬ í”¼ì²˜ì˜ ì‹¤ì§ˆì  í•œê³„ë¡œ íŒë‹¨")
        print(f"   â†’ ì¹´í…Œê³ ë¦¬ ì¬ì •ì˜ ë˜ëŠ” ì¶”ê°€ í”¼ì²˜ í•„ìš”")

print("\n" + "="*80)
print("âœ… Optuna ì‹¬í™” ì™„ë£Œ!")
print("="*80)
print(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {(optuna_time_lgb + optuna_time_xgb)/60:.1f}ë¶„")
print("="*80)
