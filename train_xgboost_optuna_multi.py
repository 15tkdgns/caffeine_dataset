"""
XGBoost Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° Multi-Objective íŠœë‹ (GPU)
Accuracyì™€ Macro F1 ë™ì‹œ ìµœì í™”
"""

import pandas as pd
import numpy as np
import xgboost as xgb
import optuna
import joblib
import json
from datetime import datetime
from sklearn.metrics import accuracy_score, f1_score
import time

print("="*80)
print("ğŸ”¬ XGBoost Multi-Objective Optuna íŠœë‹ (GPU)")
print("="*80)

# ============================================================
# 1. ë°ì´í„° ë¡œë“œ
# ============================================================
print("\n[1/5] ë°ì´í„° ë¡œë“œ")

X_train = np.load('02_data/02_augmented/X_train_smote.npy')
y_train = np.load('02_data/02_augmented/y_train_smote.npy')
X_test = np.load('02_data/02_augmented/X_test.npy')
y_test = np.load('02_data/02_augmented/y_test.npy')

print(f"  í•™ìŠµ: {len(X_train):,}, í…ŒìŠ¤íŠ¸: {len(X_test):,}")

# ìƒ˜í”Œë§ (ë¹ ë¥¸ íŠœë‹ì„ ìœ„í•´)
sample_size = min(1500000, len(X_train))
sample_idx = np.random.choice(len(X_train), sample_size, replace=False)
X_train_sample = X_train[sample_idx]
y_train_sample = y_train[sample_idx]
print(f"  íŠœë‹ìš© ìƒ˜í”Œ: {len(X_train_sample):,}ê±´ (ë¹ ë¥¸ ì‹¤í—˜)")

# ============================================================
# 2. XGBoost Multi-Objective í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
# ============================================================
print("\n[2/5] XGBoost Multi-Objective í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")

def objective_xgb_multi(trial):
    """
    Multi-Objective: Accuracy + Macro F1 + ìƒí™œ ì¹´í…Œê³ ë¦¬ F1 ê°œì„ 
    """
    params = {
        'device': 'cuda',
        'tree_method': 'hist',
        
        # íŠœë‹ ëŒ€ìƒ íŒŒë¼ë¯¸í„°
        'n_estimators': trial.suggest_int('n_estimators', 200, 600),
        'max_depth': trial.suggest_int('max_depth', 6, 20),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),
        'gamma': trial.suggest_float('gamma', 0.0, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),
        'max_delta_step': trial.suggest_int('max_delta_step', 0, 10),
        
        'random_state': 42,
        'n_jobs': -1
    }
    
    # ëª¨ë¸ í•™ìŠµ
    model = xgb.XGBClassifier(**params)
    model.fit(X_train_sample, y_train_sample)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€
    y_pred = model.predict(X_test)
    
    # Accuracyì™€ Macro F1
    accuracy = accuracy_score(y_test, y_pred)
    macro_f1 = f1_score(y_test, y_pred, average='macro')
    
    # Multi-Objective Score
    # 1. Accuracy 50% ê°€ì¤‘ì¹˜
    # 2. Macro F1 40% ê°€ì¤‘ì¹˜
    # 3. ìƒí™œ ì¹´í…Œê³ ë¦¬ F1 10% ê°€ì¤‘ì¹˜ (ì¤‘ìš”í•˜ì§€ë§Œ ê³¼ë„í•˜ì§€ ì•Šê²Œ)
    category_f1 = f1_score(y_test, y_pred, average=None)
    life_f1 = category_f1[1]  # ìƒí™œ ì¹´í…Œê³ ë¦¬
    
    balanced_score = 0.5 * accuracy + 0.4 * macro_f1 + 0.1 * life_f1
    
    return balanced_score

print("  Optuna ì‹œì‘ (60 trials - ë” ë§ì´ íƒìƒ‰)...")
print("  ëª©í‘œ: Accuracy 50% + Macro F1 40% + ìƒí™œ F1 10%")

study_xgb = optuna.create_study(direction='maximize', study_name='xgboost_gpu_multi_objective')
study_xgb.optimize(objective_xgb_multi, n_trials=60, show_progress_bar=True)

print(f"\n  âœ… ìµœì  ì ìˆ˜: {study_xgb.best_value:.4f}")
print(f"  ğŸ“‹ ìµœì  íŒŒë¼ë¯¸í„°:")
for key, value in study_xgb.best_params.items():
    if isinstance(value, float):
        print(f"     {key}: {value:.6f}")
    else:
        print(f"     {key}: {value}")

# ============================================================
# 3. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ ë°ì´í„° í•™ìŠµ
# ============================================================
print("\n[3/5] ìµœì  ëª¨ë¸ í•™ìŠµ (ì „ì²´ ë°ì´í„°)")

best_params_xgb = study_xgb.best_params.copy()
best_params_xgb['device'] = 'cuda'
best_params_xgb['tree_method'] = 'hist'
best_params_xgb['random_state'] = 42
best_params_xgb['n_jobs'] = -1

print(f"\n  [XGBoost] ì „ì²´ ë°ì´í„° í•™ìŠµ ì‹œì‘...")
start = time.time()
best_xgb = xgb.XGBClassifier(**best_params_xgb)
best_xgb.fit(X_train, y_train)
train_time = time.time() - start
print(f"  âœ… í•™ìŠµ ì™„ë£Œ: {train_time:.2f}ì´ˆ ({train_time/60:.2f}ë¶„)")

# í‰ê°€
y_pred = best_xgb.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
macro_f1 = f1_score(y_test, y_pred, average='macro')
weighted_f1 = f1_score(y_test, y_pred, average='weighted')
category_f1 = f1_score(y_test, y_pred, average=None)

print(f"\n  ğŸ“Š ìµœì¢… ì„±ëŠ¥:")
print(f"     Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"     Macro F1:    {macro_f1:.4f} ({macro_f1*100:.2f}%)")
print(f"     Weighted F1: {weighted_f1:.4f}")

category_names = ['êµí†µ', 'ìƒí™œ', 'ì‡¼í•‘', 'ì‹ë£Œí’ˆ', 'ì™¸ì‹', 'ì£¼ìœ ']
print(f"\n  ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ F1:")
for cat, f1 in zip(category_names, category_f1):
    emoji = "â­" if f1 > 0.5 else "âœ…" if f1 > 0.3 else "âš ï¸"
    print(f"     {emoji} {cat:6s}: {f1:.4f}")

# ============================================================
# 4. ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥
# ============================================================
print("\n[4/5] ëª¨ë¸ ì €ì¥")

import os
os.makedirs('03_models/xgboost_optuna_multi', exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# ëª¨ë¸ ì €ì¥
model_path = f'03_models/xgboost_optuna_multi/xgboost_multi_tuned_{timestamp}.joblib'
joblib.dump(best_xgb, model_path)
print(f"  âœ… ëª¨ë¸ ì €ì¥: {model_path}")

# ë©”íƒ€ë°ì´í„° ì €ì¥
metadata = {
    'model_info': {
        'name': 'XGBoost GPU (Multi-Objective Optuna)',
        'tuning_method': 'Optuna TPE (Multi-Objective)',
        'n_trials': 60,
        'objective': 'Balanced: 50% Acc + 40% F1 + 10% Life F1',
        'created_at': datetime.now().isoformat()
    },
    'best_params': best_params_xgb,
    'performance': {
        'accuracy': round(accuracy, 4),
        'macro_f1': round(macro_f1, 4),
        'weighted_f1': round(weighted_f1, 4),
        'category_f1': {cat: round(f1, 4) for cat, f1 in zip(category_names, category_f1)},
        'train_time_seconds': round(train_time, 2)
    },
    'comparison': {
        'baseline_lgb_acc': 0.4913,
        'baseline_lgb_f1': 0.4344,
        'baseline_life_f1': 0.0802,
        'improvement_acc': round((accuracy - 0.4913) * 100, 2),
        'improvement_f1': round((macro_f1 - 0.4344) * 100, 2),
        'improvement_life_f1': round((category_f1[1] - 0.0802) * 100, 2)
    }
}

metadata_path = f'03_models/xgboost_optuna_multi/metadata_{timestamp}.json'
with open(metadata_path, 'w', encoding='utf-8') as f:
    json.dump(metadata, f, indent=2, ensure_ascii=False)
print(f"  âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")

# ============================================================
# 5. ê²°ê³¼ ë¹„êµ
# ============================================================
print("\n" + "="*80)
print("ğŸ† XGBoost Multi-Objective íŠœë‹ ê²°ê³¼")
print("="*80)

baseline = {'accuracy': 0.4913, 'macro_f1': 0.4344, 'life_f1': 0.0802}
previous_tuning = {'accuracy': 0.4807, 'macro_f1': 0.4547, 'life_f1': 0.2051}

print(f"\n{'ëª¨ë¸':<45} {'Accuracy':>12} {'Macro F1':>12} {'ìƒí™œ F1':>12}")
print("-"*85)
print(f"{'ê¸°ì¡´ LightGBM (Baseline)':<45} {baseline['accuracy']:>12.4f} {baseline['macro_f1']:>12.4f} {baseline['life_f1']:>12.4f}")
print(f"{'ì´ì „ XGBoost Optuna (F1 ìµœì í™”)':<45} {previous_tuning['accuracy']:>12.4f} {previous_tuning['macro_f1']:>12.4f} {previous_tuning['life_f1']:>12.4f}")
print(f"{'NEW: Multi-Objective XGBoost':<45} {accuracy:>12.4f} {macro_f1:>12.4f} {category_f1[1]:>12.4f}")
print("-"*85)

acc_vs_baseline = (accuracy - baseline['accuracy']) * 100
f1_vs_baseline = (macro_f1 - baseline['macro_f1']) * 100
life_vs_baseline = (category_f1[1] - baseline['life_f1']) * 100

print(f"\nğŸ“Š Baseline ëŒ€ë¹„ ê°œì„ ë„:")
print(f"  Accuracy:    {acc_vs_baseline:+.2f}%p")
print(f"  Macro F1:    {f1_vs_baseline:+.2f}%p")
print(f"  ìƒí™œ F1:     {life_vs_baseline:+.2f}%p")

# ì¢…í•© í‰ê°€
improvements = 0
if accuracy >= baseline['accuracy']: improvements += 1
if macro_f1 >= baseline['macro_f1']: improvements += 1
if category_f1[1] >= 0.15: improvements += 1  # ìƒí™œ F1 15% ì´ìƒì´ë©´ ì„±ê³µ

print(f"\nğŸ¯ ì¢…í•© í‰ê°€:")
if improvements == 3:
    print(f"  âœ…âœ…âœ… ì™„ë²½í•œ ì„±ê³µ! ëª¨ë“  ì§€í‘œ ê°œì„ ")
elif improvements == 2:
    print(f"  âœ…âœ… ìš°ìˆ˜! ì£¼ìš” ì§€í‘œ ê°œì„ ")
elif improvements == 1:
    print(f"  âœ… ë¶€ë¶„ ì„±ê³µ")
else:
    print(f"  âš ï¸ ì¶”ê°€ íŠœë‹ í•„ìš”")

print("\n" + "="*80)
print("âœ… XGBoost Multi-Objective íŠœë‹ ì™„ë£Œ!")
print("="*80)
print(f"\nğŸ“¦ ì €ì¥ íŒŒì¼:")
print(f"   - ëª¨ë¸: {model_path}")
print(f"   - ë©”íƒ€ë°ì´í„°: {metadata_path}")
print("\n" + "="*80)
