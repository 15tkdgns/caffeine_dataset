{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f030467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ì „ì²˜ë¦¬\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "\n",
    "# ëª¨ë¸\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# í‰ê°€\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7a95d2",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ë¡œë“œ & ì‹œê°„ìˆœ ì •ë ¬\n",
    "\n",
    "**í•µì‹¬**: ì‹œê³„ì—´ ë°ì´í„°ì´ë¯€ë¡œ ë°˜ë“œì‹œ ì‹œê°„ìˆœ ì •ë ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e2c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "DATA_PATH = \"new_data/dataset_200k_ready.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# DateTime íŒŒì‹± & ì‹œê°„ìˆœ ì •ë ¬ (ì¤‘ìš”)\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "df = df.sort_values('DateTime').reset_index(drop=True)\n",
    "\n",
    "print(f\"ë°ì´í„°: {len(df):,}ê±´\")\n",
    "print(f\"ìœ ì € ìˆ˜: {df['User'].nunique()}ëª…\")\n",
    "print(f\"ì¹´í…Œê³ ë¦¬: {df['NextCategory'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b021dcd7",
   "metadata": {},
   "source": [
    "## 3. Train/Test ë¶„í•  (ì‹œê°„ìˆœ)\n",
    "\n",
    "âš ï¸ **ì¤‘ìš”**: ëœë¤ ë¶„í•  âŒ, ì‹œê°„ìˆœ ë¶„í•  âœ…\n",
    "\n",
    "- ê³¼ê±° 80% â†’ Train (í•™ìŠµ)\n",
    "- ë¯¸ë˜ 20% â†’ Test (ê²€ì¦)\n",
    "\n",
    "**ì´ìœ **: ë¯¸ë˜ ë°ì´í„°ë¡œ ê³¼ê±° ì˜ˆì¸¡í•˜ë©´ ë°ì´í„° ëˆ„ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269576e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°„ìˆœ ë¶„í•  (80:20)\n",
    "split_idx = int(len(df) * 0.8)\n",
    "\n",
    "train_df = df.iloc[:split_idx].copy()   # ê³¼ê±° 80%\n",
    "test_df = df.iloc[split_idx:].copy()    # ë¯¸ë˜ 20%\n",
    "\n",
    "print(f\"Train: {len(train_df):,}ê±´ (ê³¼ê±° ë°ì´í„°)\")\n",
    "print(f\"Test:  {len(test_df):,}ê±´ (ë¯¸ë˜ ë°ì´í„°)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152dc1b3",
   "metadata": {},
   "source": [
    "## 4. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "\n",
    "### 4-1. ì‹œê°„/ê¸ˆì•¡ ê¸°ë°˜ í”¼ì²˜\n",
    "\n",
    "í˜„ì¬ ê±°ë˜ ì‹œì ì˜ ì •ë³´ë§Œ ì‚¬ìš© â†’ ë°ì´í„° ëˆ„ì¶œ ì—†ìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    \"\"\"\n",
    "    ì‹œê°„/ê¸ˆì•¡ ê¸°ë°˜ í”¼ì²˜ ì¶”ê°€\n",
    "    - í˜„ì¬ ê±°ë˜ ì‹œì  ì •ë³´ë§Œ ì‚¬ìš© (ë°ì´í„° ëˆ„ì¶œ ì—†ìŒ)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ì£¼ë§ ì—¬ë¶€\n",
    "    df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
    "    \n",
    "    # ì‹œê°„ëŒ€ë³„ í”Œë˜ê·¸\n",
    "    df['IsLunchTime'] = ((df['Hour'] >= 11) & (df['Hour'] <= 14)).astype(int)    # ì ì‹¬ (11-14ì‹œ)\n",
    "    df['IsEvening'] = ((df['Hour'] >= 18) & (df['Hour'] <= 21)).astype(int)      # ì €ë… (18-21ì‹œ)\n",
    "    df['IsMorningRush'] = ((df['Hour'] >= 7) & (df['Hour'] <= 9)).astype(int)    # ì¶œê·¼ (7-9ì‹œ)\n",
    "    \n",
    "    # ê¸ˆì•¡ êµ¬ê°„ (ì €ê°€/ì¤‘ê°€/ê³ ê°€/ì´ˆê³ ê°€)\n",
    "    df['AmountBin'] = pd.cut(\n",
    "        df['Amount_Clean'], \n",
    "        bins=[-float('inf'), 20, 50, 100, float('inf')],\n",
    "        labels=['ì €ê°€', 'ì¤‘ê°€', 'ê³ ê°€', 'ì´ˆê³ ê°€']\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Train/Test ê°ê° ì ìš©\n",
    "train_df = add_time_features(train_df)\n",
    "test_df = add_time_features(test_df)\n",
    "\n",
    "print(\"âœ… ì‹œê°„/ê¸ˆì•¡ í”¼ì²˜ ì¶”ê°€ ì™„ë£Œ\")\n",
    "print(f\"   - IsWeekend, IsLunchTime, IsEvening, IsMorningRush, AmountBin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c627a9",
   "metadata": {},
   "source": [
    "### 4-2. ì‚¬ìš©ì í†µê³„ í”¼ì²˜ (âš ï¸ ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)\n",
    "\n",
    "**í•µì‹¬**: Trainì—ì„œë§Œ í†µê³„ ê³„ì‚° â†’ Testì— ì ìš©\n",
    "\n",
    "âŒ ì˜ëª»ëœ ë°©ë²•: ì „ì²´ ë°ì´í„°ì—ì„œ í†µê³„ ê³„ì‚° (ë¯¸ë˜ ì •ë³´ í¬í•¨)\n",
    "\n",
    "âœ… ì˜¬ë°”ë¥¸ ë°©ë²•: Trainì—ì„œë§Œ ê³„ì‚° â†’ Testì— merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db57eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš ï¸ Train ë°ì´í„°ì—ì„œë§Œ ì‚¬ìš©ì í†µê³„ ê³„ì‚°\n",
    "\n",
    "# 1. ì‚¬ìš©ìë³„ ê¸°ë³¸ í†µê³„ (Trainì—ì„œë§Œ)\n",
    "user_stats = train_df.groupby('User').agg({\n",
    "    'Amount_Clean': ['mean', 'std', 'count'],\n",
    "    'Category': lambda x: x.value_counts().index[0]  # ìµœë¹ˆ ì¹´í…Œê³ ë¦¬\n",
    "}).reset_index()\n",
    "\n",
    "user_stats.columns = ['User', 'User_AvgAmount', 'User_StdAmount', 'User_TxCount', 'User_FavCategory']\n",
    "\n",
    "print(\"ì‚¬ìš©ì í†µê³„ (Train ê¸°ì¤€):\")\n",
    "print(user_stats.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bd9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ì‚¬ìš©ìë³„ ì¹´í…Œê³ ë¦¬ ì´ìš© ë¹„ìœ¨ (Trainì—ì„œë§Œ)\n",
    "cat_freq = train_df.groupby(['User', 'Category']).size().unstack(fill_value=0)\n",
    "cat_freq = cat_freq.div(cat_freq.sum(axis=1), axis=0)  # ë¹„ìœ¨ë¡œ ë³€í™˜\n",
    "cat_freq.columns = [f'User_{c}_Ratio' for c in cat_freq.columns]\n",
    "cat_freq = cat_freq.reset_index()\n",
    "\n",
    "print(\"ì¹´í…Œê³ ë¦¬ë³„ ì´ìš© ë¹„ìœ¨:\")\n",
    "print(cat_freq.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train/Testì— í†µê³„ ì ìš©\n",
    "#    - Train: Train í†µê³„ ì ìš©\n",
    "#    - Test:  Train í†µê³„ ì ìš© (ë¯¸ë˜ ì •ë³´ ì‚¬ìš© âŒ)\n",
    "\n",
    "train_df = train_df.merge(user_stats, on='User', how='left')\n",
    "train_df = train_df.merge(cat_freq, on='User', how='left')\n",
    "\n",
    "test_df = test_df.merge(user_stats, on='User', how='left')  # Train í†µê³„ë¥¼ Testì— ì ìš©\n",
    "test_df = test_df.merge(cat_freq, on='User', how='left')\n",
    "\n",
    "# 4. ëˆ„ë½ê°’ ì²˜ë¦¬ (Testì— ìƒˆ ì‚¬ìš©ìê°€ ìˆì„ ê²½ìš°)\n",
    "for col in test_df.columns:\n",
    "    if col.startswith('User_') and col.endswith('_Ratio'):\n",
    "        test_df[col] = test_df[col].fillna(0)\n",
    "    elif col in ['User_AvgAmount', 'User_StdAmount', 'User_TxCount']:\n",
    "        test_df[col] = test_df[col].fillna(test_df[col].median())\n",
    "\n",
    "print(\"âœ… ì‚¬ìš©ì í†µê³„ í”¼ì²˜ ì ìš© ì™„ë£Œ\")\n",
    "print(\"   âš ï¸ Trainì—ì„œë§Œ ê³„ì‚° â†’ Testì— ì ìš© (ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7242db7e",
   "metadata": {},
   "source": [
    "## 5. í”¼ì²˜ ì •ì˜ (ì´ 21ê°œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e492b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”¼ì²˜ ì •ì˜ (ì´ 21ê°œ)\n",
    "\n",
    "# ê¸°ì¡´ í”¼ì²˜ (6ê°œ)\n",
    "base_numeric = ['Hour', 'DayOfWeek', 'Amount_Clean', 'TimeSincePrev']\n",
    "base_categorical = ['Category', 'TimeSlot']\n",
    "\n",
    "# ì‹œê°„/ê¸ˆì•¡ í”¼ì²˜ (5ê°œ) - ìƒˆë¡œ ì¶”ê°€\n",
    "time_numeric = ['IsWeekend', 'IsLunchTime', 'IsEvening', 'IsMorningRush']\n",
    "time_categorical = ['AmountBin']\n",
    "\n",
    "# ì‚¬ìš©ì í†µê³„ í”¼ì²˜ (10ê°œ) - ìƒˆë¡œ ì¶”ê°€\n",
    "user_numeric = ['User_AvgAmount', 'User_StdAmount', 'User_TxCount']\n",
    "user_numeric += [c for c in train_df.columns if c.startswith('User_') and c.endswith('_Ratio')]\n",
    "user_categorical = ['User_FavCategory']\n",
    "\n",
    "# ì „ì²´ í”¼ì²˜\n",
    "all_numeric = base_numeric + time_numeric + user_numeric\n",
    "all_categorical = base_categorical + time_categorical + user_categorical\n",
    "\n",
    "print(f\"ğŸ“Š í”¼ì²˜ êµ¬ì„±:\")\n",
    "print(f\"   - ê¸°ì¡´: {len(base_numeric + base_categorical)}ê°œ\")\n",
    "print(f\"   - ì‹œê°„/ê¸ˆì•¡: +{len(time_numeric + time_categorical)}ê°œ\")\n",
    "print(f\"   - ì‚¬ìš©ì í†µê³„: +{len(user_numeric + user_categorical)}ê°œ\")\n",
    "print(f\"   - ì´: {len(all_numeric + all_categorical)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b213eb",
   "metadata": {},
   "source": [
    "## 6. ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd616b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X (í”¼ì²˜)\n",
    "X_train = train_df[all_numeric + all_categorical]\n",
    "X_test = test_df[all_numeric + all_categorical]\n",
    "\n",
    "# y (íƒ€ê²Ÿ) - ë ˆì´ë¸” ì¸ì½”ë”©\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_df['NextCategory'])\n",
    "y_test = label_encoder.transform(test_df['NextCategory'])\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}\")\n",
    "print(f\"í´ë˜ìŠ¤: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663b34d",
   "metadata": {},
   "source": [
    "## 7. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "- ìˆ˜ì¹˜í˜•: ê²°ì¸¡ì¹˜ â†’ ì¤‘ì•™ê°’, ìŠ¤ì¼€ì¼ë§\n",
    "- ë²”ì£¼í˜•: ê²°ì¸¡ì¹˜ â†’ ìµœë¹ˆê°’, ì›í•«ì¸ì½”ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(model):\n",
    "    \"\"\"\n",
    "    ì „ì²˜ë¦¬ + ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
    "    \"\"\"\n",
    "    transformers = [\n",
    "        # ìˆ˜ì¹˜í˜• í”¼ì²˜: ê²°ì¸¡ì¹˜(ì¤‘ì•™ê°’) â†’ ìŠ¤ì¼€ì¼ë§\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')), \n",
    "            ('scaler', StandardScaler())\n",
    "        ]), all_numeric),\n",
    "        \n",
    "        # ë²”ì£¼í˜• í”¼ì²˜: ê²°ì¸¡ì¹˜(ìµœë¹ˆê°’) â†’ ì›í•«ì¸ì½”ë”©\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')), \n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ]), all_categorical)\n",
    "    ]\n",
    "    \n",
    "    return Pipeline([\n",
    "        ('preprocess', ColumnTransformer(transformers)), \n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "print(\"âœ… íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e104f",
   "metadata": {},
   "source": [
    "## 8. ëª¨ë¸ ì •ì˜ (8ê°œ)\n",
    "\n",
    "**í•µì‹¬ ì„¤ì •**: `class_weight='balanced'`\n",
    "- ë¶ˆê· í˜• ë°ì´í„° ìë™ ë³´ì •\n",
    "- ì†Œìˆ˜ í´ë˜ìŠ¤(ì‡¼í•‘ 5.7%)ì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64547a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8ê°œ ëª¨ë¸ ì •ì˜\n",
    "models = {\n",
    "    # LightGBM ê³„ì—´\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        n_estimators=300, \n",
    "        learning_rate=0.05, \n",
    "        max_depth=6, \n",
    "        class_weight='balanced',  # ë¶ˆê· í˜• ë³´ì •\n",
    "        random_state=42, \n",
    "        verbose=-1\n",
    "    ),\n",
    "    \n",
    "    'LightGBM_XL': LGBMClassifier(\n",
    "        n_estimators=600, \n",
    "        learning_rate=0.015, \n",
    "        max_depth=15, \n",
    "        num_leaves=128,\n",
    "        class_weight='balanced',\n",
    "        random_state=42, \n",
    "        verbose=-1\n",
    "    ),\n",
    "    \n",
    "    # XGBoost ê³„ì—´\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=300, \n",
    "        learning_rate=0.05, \n",
    "        max_depth=6, \n",
    "        random_state=42, \n",
    "        verbosity=0\n",
    "    ),\n",
    "    \n",
    "    'XGBoost_Deep': XGBClassifier(\n",
    "        n_estimators=400, \n",
    "        learning_rate=0.03, \n",
    "        max_depth=10, \n",
    "        random_state=42, \n",
    "        verbosity=0\n",
    "    ),\n",
    "    \n",
    "    # RandomForest ê³„ì—´\n",
    "    'RandomForest': RandomForestClassifier(\n",
    "        n_estimators=200, \n",
    "        max_depth=15, \n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    'RandomForest_Deep': RandomForestClassifier(\n",
    "        n_estimators=300, \n",
    "        max_depth=25, \n",
    "        class_weight='balanced_subsample',\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    # HistGradientBoosting (ë¹ ë¦„)\n",
    "    'HistGB': HistGradientBoostingClassifier(\n",
    "        max_iter=300, \n",
    "        learning_rate=0.05, \n",
    "        max_depth=10, \n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    # ExtraTrees (ğŸ† ìµœê³  ì„±ëŠ¥)\n",
    "    'ExtraTrees': ExtraTreesClassifier(\n",
    "        n_estimators=200, \n",
    "        max_depth=15, \n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"âœ… {len(models)}ê°œ ëª¨ë¸ ì •ì˜ ì™„ë£Œ\")\n",
    "for name in models:\n",
    "    print(f\"   - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584b3938",
   "metadata": {},
   "source": [
    "## 9. ëª¨ë¸ í•™ìŠµ & í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2964bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'ëª¨ë¸':>20} | {'Accuracy':>10} | {'Macro F1':>10} | {'ì‹œê°„':>8}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    start = time.time()\n",
    "    \n",
    "    # íŒŒì´í”„ë¼ì¸ ìƒì„± & í•™ìŠµ\n",
    "    pipeline = build_pipeline(model)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # í‰ê°€\n",
    "    acc = accuracy_score(y_test, y_pred) * 100\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro') * 100\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name, \n",
    "        'Accuracy': acc, \n",
    "        'Macro_F1': macro_f1, \n",
    "        'Time': elapsed\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:>20} | {acc:>9.2f}% | {macro_f1:>9.2f}% | {elapsed:>7.2f}s\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b84ebe",
   "metadata": {},
   "source": [
    "## 10. ìµœì¢… ê²°ê³¼ (Macro F1 ê¸°ì¤€ ì •ë ¬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ed40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì •ë ¬\n",
    "results_df = pd.DataFrame(results).sort_values('Macro_F1', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ† ìµœì¢… ìˆœìœ„ (Macro F1 ê¸°ì¤€)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, (_, row) in enumerate(results_df.iterrows(), 1):\n",
    "    marker = \" â† BEST!\" if i == 1 else \"\"\n",
    "    print(f\"{i}. {row['Model']:20s} | Accuracy: {row['Accuracy']:.2f}% | Macro F1: {row['Macro_F1']:.2f}%{marker}\")\n",
    "\n",
    "# í‘œë¡œ ì¶œë ¥\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557b03d",
   "metadata": {},
   "source": [
    "## 11. ìµœê³  ëª¨ë¸ ìƒì„¸ ë¶„ì„ (ExtraTrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81620771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœê³  ëª¨ë¸ë¡œ ë‹¤ì‹œ í•™ìŠµ\n",
    "best_model = ExtraTreesClassifier(\n",
    "    n_estimators=200, \n",
    "    max_depth=15, \n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipeline = build_pipeline(best_model)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥\n",
    "print(\"ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ (ExtraTrees)\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f03b8fd",
   "metadata": {},
   "source": [
    "## 12. í”¼ì²˜ ì¤‘ìš”ë„ (Feature Importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f569f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”¼ì²˜ ì´ë¦„ ì¶”ì¶œ\n",
    "preprocessor = pipeline.named_steps['preprocess']\n",
    "feature_names = []\n",
    "\n",
    "for name, transformer, columns in preprocessor.transformers_:\n",
    "    if name == 'num':\n",
    "        feature_names.extend(columns)\n",
    "    elif name == 'cat':\n",
    "        encoder = transformer.named_steps['encoder']\n",
    "        cat_features = encoder.get_feature_names_out(columns)\n",
    "        feature_names.extend(cat_features)\n",
    "\n",
    "# ì¤‘ìš”ë„ ì¶”ì¶œ\n",
    "importances = pipeline.named_steps['model'].feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Top 15 ì¶œë ¥\n",
    "print(\"ğŸ“Š í”¼ì²˜ ì¤‘ìš”ë„ Top 15\")\n",
    "print(\"=\" * 50)\n",
    "for i, (_, row) in enumerate(importance_df.head(15).iterrows(), 1):\n",
    "    print(f\"{i:2d}. {row['Feature']:30s}: {row['Importance']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b755fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™” (matplotlib)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top15 = importance_df.head(15).sort_values('Importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(top15['Feature'], top15['Importance'] * 100, color='steelblue')\n",
    "plt.xlabel('Importance (%)')\n",
    "plt.title('Feature Importance Top 15 (ExtraTrees)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af7cc73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Œ í•µì‹¬ ìš”ì•½\n",
    "\n",
    "### 1. íƒ€ê²Ÿ ìƒì„± (ë‹¤ìŒ ë¼ë²¨ ì˜ˆì¸¡)\n",
    "```python\n",
    "df['NextCategory'] = df.groupby('User')['Category'].shift(-1)\n",
    "```\n",
    "- `shift(-1)`: ë‹¤ìŒ í–‰ì˜ ì¹´í…Œê³ ë¦¬ â†’ ë¯¸ë˜ ì˜ˆì¸¡!\n",
    "\n",
    "### 2. ë°ì´í„° ëˆ„ì¶œ ë°©ì§€\n",
    "- **ì‹œê°„ìˆœ ë¶„í• **: ê³¼ê±° 80% â†’ Train, ë¯¸ë˜ 20% â†’ Test\n",
    "- **ì‚¬ìš©ì í†µê³„**: Trainì—ì„œë§Œ ê³„ì‚° â†’ Testì— ì ìš©\n",
    "\n",
    "### 3. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (+5.33%p í–¥ìƒ)\n",
    "- ê¸°ì¡´ 6ê°œ â†’ 21ê°œ í”¼ì²˜\n",
    "- ì‚¬ìš©ì í†µê³„ í”¼ì²˜ê°€ íš¨ê³¼ì !\n",
    "\n",
    "### 4. ìµœì¢… ì„±ê³¼\n",
    "| í•­ëª© | ê°’ |\n",
    "|------|-----|\n",
    "| ëª¨ë¸ | ExtraTrees |\n",
    "| Macro F1 | **54.86%** |\n",
    "| Accuracy | **63.09%** |\n",
    "| í”¼ì²˜ ìˆ˜ | 21ê°œ |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
