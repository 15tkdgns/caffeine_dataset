"""
Class Weight ì¡°ì •ìœ¼ë¡œ ìƒí™œ ì¹´í…Œê³ ë¦¬ ì„±ëŠ¥ ê°œì„ 
ë¶ˆê· í˜• í´ë˜ìŠ¤ì— ì§‘ì¤‘í•˜ì—¬ Macro F1 ìµœì í™”
"""

import pandas as pd
import numpy as np
import xgboost as xgb
import lightgbm as lgb
from sklearn.utils.class_weight import compute_class_weight
import joblib
import json
from datetime import datetime
from sklearn.metrics import accuracy_score, f1_score, classification_report
import time

print("="*80)
print("âš–ï¸ Class Weight ì¡°ì •ìœ¼ë¡œ ë¶ˆê· í˜• ê°œì„  (GPU)")
print("="*80)

# ============================================================
# 1. ë°ì´í„° ë¡œë“œ
# ============================================================
print("\n[1/5] ë°ì´í„° ë¡œë“œ")

X_train = np.load('02_data/02_augmented/X_train_smote.npy')
y_train = np.load('02_data/02_augmented/y_train_smote.npy')
X_test = np.load('02_data/02_augmented/X_test.npy')
y_test = np.load('02_data/02_augmented/y_test.npy')

print(f"  í•™ìŠµ: {len(X_train):,}, í…ŒìŠ¤íŠ¸: {len(X_test):,}")

# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
unique, counts = np.unique(y_test, return_counts=True)
category_names = ['êµí†µ', 'ìƒí™œ', 'ì‡¼í•‘', 'ì‹ë£Œí’ˆ', 'ì™¸ì‹', 'ì£¼ìœ ']
print(f"\n  í…ŒìŠ¤íŠ¸ ë°ì´í„° í´ë˜ìŠ¤ ë¶„í¬:")
for cat_id, cat_name, count in zip(unique, category_names, counts):
    print(f"     {cat_name:6s}: {count:,}ê±´ ({count/len(y_test)*100:.1f}%)")

# ============================================================
# 2. Class Weight ê³„ì‚°
# ============================================================
print("\n[2/5] Class Weight ê³„ì‚°")

# sklearnìœ¼ë¡œ ìë™ ê³„ì‚°
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)

# ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
class_weight_dict = {i: w for i, w in enumerate(class_weights)}

print(f"  Class Weights (ìë™ ê³„ì‚°):")
for cat_id, cat_name, weight in zip(unique, category_names, class_weights):
    print(f"     {cat_name:6s}: {weight:.4f}")

# ìƒí™œ ì¹´í…Œê³ ë¦¬ ê°€ì¤‘ì¹˜ ì¶”ê°€ ë¶€ìŠ¤íŒ…
class_weight_dict[1] *= 5.0  # ìƒí™œ ì¹´í…Œê³ ë¦¬ (F1 8%)ë¥¼ 5ë°° ê°•ì¡°

print(f"\n  ìƒí™œ ì¹´í…Œê³ ë¦¬ ë¶€ìŠ¤íŒ… í›„:")
for cat_id, cat_name in enumerate(category_names):
    print(f"     {cat_name:6s}: {class_weight_dict[cat_id]:.4f}")

# Sample weight ìƒì„±
sample_weights = np.array([class_weight_dict[y] for y in y_train])

# ============================================================
# 3. XGBoost with Class Weight (GPU)
# ============================================================
print("\n[3/5] XGBoost with Class Weight")

print("  í•™ìŠµ ì‹œì‘...")
start = time.time()

xgb_weighted = xgb.XGBClassifier(
    device='cuda',
    tree_method='hist',
    n_estimators=300,
    max_depth=10,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)

xgb_weighted.fit(X_train, y_train, sample_weight=sample_weights)
train_time_xgb = time.time() - start

# í‰ê°€
y_pred_xgb = xgb_weighted.predict(X_test)
acc_xgb = accuracy_score(y_test, y_pred_xgb)
f1_xgb = f1_score(y_test, y_pred_xgb, average='macro')
weighted_f1_xgb = f1_score(y_test, y_pred_xgb, average='weighted')
category_f1_xgb = f1_score(y_test, y_pred_xgb, average=None)

print(f"  âœ… ì™„ë£Œ: {train_time_xgb:.2f}ì´ˆ")
print(f"\n  ğŸ“Š ì„±ëŠ¥:")
print(f"     Accuracy:    {acc_xgb:.4f} ({acc_xgb*100:.2f}%)")
print(f"     Macro F1:    {f1_xgb:.4f} ({f1_xgb*100:.2f}%)")
print(f"     Weighted F1: {weighted_f1_xgb:.4f}")
print(f"\n  ì¹´í…Œê³ ë¦¬ë³„ F1:")
for cat_name, f1 in zip(category_names, category_f1_xgb):
    print(f"     {cat_name:6s}: {f1:.4f}")

# ============================================================
# 4. LightGBM with Class Weight
# ============================================================
print("\n[4/5] LightGBM with Class Weight")

print("  í•™ìŠµ ì‹œì‘...")
start = time.time()

lgb_weighted = lgb.LGBMClassifier(
    n_estimators=300,
    max_depth=10,
    learning_rate=0.1,
    num_leaves=128,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
    verbose=-1
)

lgb_weighted.fit(X_train, y_train, sample_weight=sample_weights)
train_time_lgb = time.time() - start

# í‰ê°€
y_pred_lgb = lgb_weighted.predict(X_test)
acc_lgb = accuracy_score(y_test, y_pred_lgb)
f1_lgb = f1_score(y_test, y_pred_lgb, average='macro')
weighted_f1_lgb = f1_score(y_test, y_pred_lgb, average='weighted')
category_f1_lgb = f1_score(y_test, y_pred_lgb, average=None)

print(f"  âœ… ì™„ë£Œ: {train_time_lgb:.2f}ì´ˆ")
print(f"\n  ğŸ“Š ì„±ëŠ¥:")
print(f"     Accuracy:    {acc_lgb:.4f} ({acc_lgb*100:.2f}%)")
print(f"     Macro F1:    {f1_lgb:.4f} ({f1_lgb*100:.2f}%)")
print(f"     Weighted F1: {weighted_f1_lgb:.4f}")
print(f"\n  ì¹´í…Œê³ ë¦¬ë³„ F1:")
for cat_name, f1 in zip(category_names, category_f1_lgb):
    print(f"     {cat_name:6s}: {f1:.4f}")

# ============================================================
# 5. ê²°ê³¼ ì €ì¥ ë° ë¹„êµ
# ============================================================
print("\n[5/5] ê²°ê³¼ ì €ì¥")

import os
os.makedirs('03_models/class_weighted', exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
if f1_xgb > f1_lgb:
    best_model = xgb_weighted
    best_name = 'XGBoost (Class Weighted)'
    best_acc = acc_xgb
    best_f1 = f1_xgb
    best_category_f1 = category_f1_xgb
    model_type = 'xgboost'
else:
    best_model = lgb_weighted
    best_name = 'LightGBM (Class Weighted)'
    best_acc = acc_lgb
    best_f1 = f1_lgb
    best_category_f1 = category_f1_lgb
    model_type = 'lightgbm'

# ëª¨ë¸ ì €ì¥
model_path = f'03_models/class_weighted/{model_type}_weighted_{timestamp}.joblib'
joblib.dump(best_model, model_path)
print(f"  âœ… ëª¨ë¸ ì €ì¥: {model_path}")

# ë©”íƒ€ë°ì´í„°
metadata = {
    'model_info': {
        'name': best_name,
        'method': 'Class Weight (Balanced + Life x5)',
        'created_at': datetime.now().isoformat()
    },
    'class_weights': {cat: float(class_weight_dict[i]) for i, cat in enumerate(category_names)},
    'performance': {
        'accuracy': round(best_acc, 4),
        'macro_f1': round(best_f1, 4),
        'category_f1': {cat: round(f1, 4) for cat, f1 in zip(category_names, best_category_f1)}
    },
    'comparison': {
        'xgboost': {
            'accuracy': round(acc_xgb, 4),
            'macro_f1': round(f1_xgb, 4),
            'life_f1': round(category_f1_xgb[1], 4)
        },
        'lightgbm': {
            'accuracy': round(acc_lgb, 4),
            'macro_f1': round(f1_lgb, 4),
            'life_f1': round(category_f1_lgb[1], 4)
        }
    }
}

metadata_path = f'03_models/class_weighted/metadata_{timestamp}.json'
with open(metadata_path, 'w', encoding='utf-8') as f:
    json.dump(metadata, f, indent=2, ensure_ascii=False)
print(f"  âœ… ë©”íƒ€ë°ì´í„°: {metadata_path}")

# ============================================================
# ë¹„êµ ê²°ê³¼
# ============================================================
print("\n" + "="*80)
print("ğŸ† Class Weight ì¡°ì • ê²°ê³¼")
print("="*80)

baseline = {'acc': 0.4913, 'f1': 0.4344, 'life_f1': 0.0802}

print(f"\n{'ëª¨ë¸':<35} {'Accuracy':>12} {'Macro F1':>12} {'ìƒí™œ F1':>12}")
print("-"*75)
print(f"{'Baseline (LightGBM)':<35} {baseline['acc']:>12.4f} {baseline['f1']:>12.4f} {baseline['life_f1']:>12.4f}")
print(f"{'XGBoost (Class Weighted)':<35} {acc_xgb:>12.4f} {f1_xgb:>12.4f} {category_f1_xgb[1]:>12.4f}")
print(f"{'LightGBM (Class Weighted)':<35} {acc_lgb:>12.4f} {f1_lgb:>12.4f} {category_f1_lgb[1]:>12.4f}")
print("-"*75)

print(f"\nìƒí™œ ì¹´í…Œê³ ë¦¬ ê°œì„ :")
print(f"  XGBoost:  {baseline['life_f1']:.4f} â†’ {category_f1_xgb[1]:.4f} ({(category_f1_xgb[1] - baseline['life_f1'])*100:+.2f}%p)")
print(f"  LightGBM: {baseline['life_f1']:.4f} â†’ {category_f1_lgb[1]:.4f} ({(category_f1_lgb[1] - baseline['life_f1'])*100:+.2f}%p)")

if category_f1_xgb[1] > baseline['life_f1'] or category_f1_lgb[1] > baseline['life_f1']:
    print(f"\nâœ… ìƒí™œ ì¹´í…Œê³ ë¦¬ ì„±ëŠ¥ ê°œì„  ì„±ê³µ!")
else:
    print(f"\nâš ï¸ ì¶”ê°€ ì¡°ì • í•„ìš”")

print("\n" + "="*80)
print("âœ… Class Weight ì‹¤í—˜ ì™„ë£Œ!")
print("="*80)
