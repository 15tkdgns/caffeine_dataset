"""
Top ëª¨ë¸ ì•™ìƒë¸” ìµœì í™”
LightGBM + XGBoost ì¡°í•©ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±
"""

import pandas as pd
import numpy as np
import joblib
import json
from datetime import datetime
from sklearn.metrics import accuracy_score, f1_score
from sklearn.ensemble import VotingClassifier
import optuna
import time

print("="*80)
print("ğŸ¯ Top ëª¨ë¸ ì•™ìƒë¸” ìµœì í™”")
print("="*80)

# ============================================================
# 1. ë°ì´í„° ë¡œë“œ
# ============================================================
print("\n[1/6] ë°ì´í„° ë¡œë“œ")

X_train = np.load('02_data/02_augmented/X_train_smote.npy')
y_train = np.load('02_data/02_augmented/y_train_smote.npy')
X_test = np.load('02_data/02_augmented/X_test.npy')
y_test = np.load('02_data/02_augmented/y_test.npy')

print(f"  í•™ìŠµ: {len(X_train):,}, í…ŒìŠ¤íŠ¸: {len(X_test):,}")

# ============================================================
# 2. ê¸°ì¡´ Top ëª¨ë¸ ë¡œë“œ
# ============================================================
print("\n[2/6] Top ëª¨ë¸ ë¡œë“œ")

# Baseline LightGBM
try:
    lgb_model = joblib.load('03_models/production_models/lightgbm_cuda_production_20251205_162340.joblib')
    print("  âœ… Baseline LightGBM ë¡œë“œ ì™„ë£Œ")
    has_lgb = True
except:
    print("  âš ï¸ Baseline LightGBM ë¡œë“œ ì‹¤íŒ¨")
    has_lgb = False

# Multi-Objective XGBoost
try:
    xgb_multi = joblib.load('03_models/xgboost_optuna_multi/xgboost_multi_tuned_20251207_044613.joblib')
    print("  âœ… Multi-Objective XGBoost ë¡œë“œ ì™„ë£Œ")
    has_xgb_multi = True
except:
    print("  âš ï¸ Multi-Objective XGBoost ë¡œë“œ ì‹¤íŒ¨")
    has_xgb_multi = False

# ì´ì „ Optuna XGBoost
try:
    xgb_optuna = joblib.load('03_models/optuna_tuned/xgboost_tuned_20251205_184240.joblib')
    print("  âœ… ì´ì „ Optuna XGBoost ë¡œë“œ ì™„ë£Œ")
    has_xgb_optuna = True
except:
    print("  âš ï¸ ì´ì „ Optuna XGBoost ë¡œë“œ ì‹¤íŒ¨")
    has_xgb_optuna = False

# ============================================================
# 3. ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
# ============================================================
print("\n[3/6] ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")

models_info = []

if has_lgb:
    y_pred_lgb = lgb_model.predict(X_test)
    acc_lgb = accuracy_score(y_test, y_pred_lgb)
    f1_lgb = f1_score(y_test, y_pred_lgb, average='macro')
    print(f"\n  LightGBM Baseline:")
    print(f"    Accuracy: {acc_lgb:.4f}, Macro F1: {f1_lgb:.4f}")
    models_info.append(('lgb', lgb_model, acc_lgb, f1_lgb))

if has_xgb_multi:
    y_pred_xgb_multi = xgb_multi.predict(X_test)
    acc_xgb_multi = accuracy_score(y_test, y_pred_xgb_multi)
    f1_xgb_multi = f1_score(y_test, y_pred_xgb_multi, average='macro')
    print(f"\n  XGBoost Multi-Objective:")
    print(f"    Accuracy: {acc_xgb_multi:.4f}, Macro F1: {f1_xgb_multi:.4f}")
    models_info.append(('xgb_multi', xgb_multi, acc_xgb_multi, f1_xgb_multi))

if has_xgb_optuna:
    y_pred_xgb_optuna = xgb_optuna.predict(X_test)
    acc_xgb_optuna = accuracy_score(y_test, y_pred_xgb_optuna)
    f1_xgb_optuna = f1_score(y_test, y_pred_xgb_optuna, average='macro')
    print(f"\n  XGBoost Optuna (ì´ì „):")
    print(f"    Accuracy: {acc_xgb_optuna:.4f}, Macro F1: {f1_xgb_optuna:.4f}")
    models_info.append(('xgb_optuna', xgb_optuna, acc_xgb_optuna, f1_xgb_optuna))

# ============================================================
# 4. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”
# ============================================================
print("\n[4/6] ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™” (Optuna)")

# ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥  ì €ì¥
proba_dict = {}
if has_lgb:
    proba_dict['lgb'] = lgb_model.predict_proba(X_test)
if has_xgb_multi:
    proba_dict['xgb_multi'] = xgb_multi.predict_proba(X_test)
if has_xgb_optuna:
    proba_dict['xgb_optuna'] = xgb_optuna.predict_proba(X_test)

def objective_ensemble(trial):
    """
    ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”
    """
    weights = []
    
    if has_lgb:
        w_lgb = trial.suggest_float('w_lgb', 0.0, 1.0)
        weights.append(w_lgb)
    else:
        w_lgb = 0
        
    if has_xgb_multi:
        w_xgb_multi = trial.suggest_float('w_xgb_multi', 0.0, 1.0)
        weights.append(w_xgb_multi)
    else:
        w_xgb_multi = 0
        
    if has_xgb_optuna:
        w_xgb_optuna = trial.suggest_float('w_xgb_optuna', 0.0, 1.0)
        weights.append(w_xgb_optuna)
    else:
        w_xgb_optuna = 0
    
    # ê°€ì¤‘ì¹˜ ì •ê·œí™”
    total_weight = sum(weights)
    if total_weight == 0:
        return 0
    
    weights = [w / total_weight for w in weights]
    
    # ê°€ì¤‘ í‰ê·  ì˜ˆì¸¡
    ensemble_proba = np.zeros_like(list(proba_dict.values())[0])
    
    idx = 0
    if has_lgb:
        ensemble_proba += weights[idx] * proba_dict['lgb']
        idx += 1
    if has_xgb_multi:
        ensemble_proba += weights[idx] * proba_dict['xgb_multi']
        idx += 1
    if has_xgb_optuna:
        ensemble_proba += weights[idx] * proba_dict['xgb_optuna']
        idx += 1
    
    # ìµœì¢… ì˜ˆì¸¡
    y_pred = np.argmax(ensemble_proba, axis=1)
    
    # Multi-Objective: Accuracy 60% + Macro F1 40%
    accuracy = accuracy_score(y_test, y_pred)
    macro_f1 = f1_score(y_test, y_pred, average='macro')
    
    score = 0.6 * accuracy + 0.4 * macro_f1
    
    return score

print("  Optuna ì‹œì‘ (30 trials)...")
study_ensemble = optuna.create_study(direction='maximize', study_name='ensemble_weights')
study_ensemble.optimize(objective_ensemble, n_trials=30, show_progress_bar=True)

print(f"\n  âœ… ìµœì  ì ìˆ˜: {study_ensemble.best_value:.4f}")
print(f"  ğŸ“‹ ìµœì  ê°€ì¤‘ì¹˜:")
for key, value in study_ensemble.best_params.items():
    print(f"     {key}: {value:.4f}")

# ============================================================
# 5. ìµœì  ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ë° í‰ê°€
# ============================================================
print("\n[5/6] ìµœì  ì•™ìƒë¸” ëª¨ë¸ í‰ê°€")

# ìµœì  ê°€ì¤‘ì¹˜ ì¶”ì¶œ
best_weights = []
if has_lgb:
    best_weights.append(study_ensemble.best_params['w_lgb'])
if has_xgb_multi:
    best_weights.append(study_ensemble.best_params['w_xgb_multi'])
if has_xgb_optuna:
    best_weights.append(study_ensemble.best_params['w_xgb_optuna'])

# ê°€ì¤‘ì¹˜ ì •ê·œí™”
total = sum(best_weights)
best_weights = [w / total for w in best_weights]

print(f"\n  ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜:")
idx = 0
if has_lgb:
    print(f"    LightGBM: {best_weights[idx]:.4f}")
    idx += 1
if has_xgb_multi:
    print(f"    XGBoost Multi: {best_weights[idx]:.4f}")
    idx += 1
if has_xgb_optuna:
    print(f"    XGBoost Optuna: {best_weights[idx]:.4f}")
    idx += 1

# ê°€ì¤‘ í‰ê·  ì˜ˆì¸¡
ensemble_proba = np.zeros_like(list(proba_dict.values())[0])
idx = 0
if has_lgb:
    ensemble_proba += best_weights[idx] * proba_dict['lgb']
    idx += 1
if has_xgb_multi:
    ensemble_proba += best_weights[idx] * proba_dict['xgb_multi']
    idx += 1
if has_xgb_optuna:
    ensemble_proba += best_weights[idx] * proba_dict['xgb_optuna']
    idx += 1

y_pred_ensemble = np.argmax(ensemble_proba, axis=1)

# ìµœì¢… ì„±ëŠ¥
accuracy = accuracy_score(y_test, y_pred_ensemble)
macro_f1 = f1_score(y_test, y_pred_ensemble, average='macro')
weighted_f1 = f1_score(y_test, y_pred_ensemble, average='weighted')
category_f1 = f1_score(y_test, y_pred_ensemble, average=None)

print(f"\n  ğŸ“Š ì•™ìƒë¸” ìµœì¢… ì„±ëŠ¥:")
print(f"     Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"     Macro F1:    {macro_f1:.4f} ({macro_f1*100:.2f}%)")
print(f"     Weighted F1: {weighted_f1:.4f}")

category_names = ['êµí†µ', 'ìƒí™œ', 'ì‡¼í•‘', 'ì‹ë£Œí’ˆ', 'ì™¸ì‹', 'ì£¼ìœ ']
print(f"\n  ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ F1:")
for cat, f1 in zip(category_names, category_f1):
    emoji = "â­" if f1 > 0.5 else "âœ…" if f1 > 0.3 else "âš ï¸"
    print(f"     {emoji} {cat:6s}: {f1:.4f}")

# ============================================================
# 6. ê²°ê³¼ ì €ì¥
# ============================================================
print("\n[6/6] ê²°ê³¼ ì €ì¥")

import os
os.makedirs('03_models/ensemble_optimized', exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# ë©”íƒ€ë°ì´í„° ì €ì¥
metadata = {
    'ensemble_info': {
        'name': 'Weighted Ensemble (Optimized)',
        'method': 'Soft Voting with Optuna-optimized weights',
        'n_models': len(models_info),
        'created_at': datetime.now().isoformat()
    },
    'weights': {
        'lgb': best_weights[0] if has_lgb else 0,
        'xgb_multi': best_weights[1 if has_lgb else 0] if has_xgb_multi else 0,
        'xgb_optuna': best_weights[-1] if has_xgb_optuna else 0
    },
    'performance': {
        'accuracy': round(accuracy, 4),
        'macro_f1': round(macro_f1, 4),
        'weighted_f1': round(weighted_f1, 4),
        'category_f1': {cat: round(f1, 4) for cat, f1 in zip(category_names, category_f1)}
    },
    'individual_models': {
        'lgb': {'acc': round(acc_lgb, 4), 'f1': round(f1_lgb, 4)} if has_lgb else None,
        'xgb_multi': {'acc': round(acc_xgb_multi, 4), 'f1': round(f1_xgb_multi, 4)} if has_xgb_multi else None,
        'xgb_optuna': {'acc': round(acc_xgb_optuna, 4), 'f1': round(f1_xgb_optuna, 4)} if has_xgb_optuna else None
    },
    'comparison': {
        'baseline_lgb_acc': 0.4913,
        'baseline_lgb_f1': 0.4344,
        'improvement_acc': round((accuracy - 0.4913) * 100, 2),
        'improvement_f1': round((macro_f1 - 0.4344) * 100, 2)
    }
}

metadata_path = f'03_models/ensemble_optimized/metadata_{timestamp}.json'
with open(metadata_path, 'w', encoding='utf-8') as f:
    json.dump(metadata, f, indent=2, ensure_ascii=False)
print(f"  âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")

# ============================================================
# 7. ìµœì¢… ë¹„êµ
# ============================================================
print("\n" + "="*80)
print("ğŸ† ì•™ìƒë¸” ìµœì í™” ê²°ê³¼")
print("="*80)

baseline = {'accuracy': 0.4913, 'macro_f1': 0.4344, 'life_f1': 0.0802}

print(f"\n{'ëª¨ë¸':<50} {'Accuracy':>12} {'Macro F1':>12} {'ìƒí™œ F1':>12}")
print("-"*90)
print(f"{'ê¸°ì¡´ LightGBM (Baseline)':<50} {baseline['accuracy']:>12.4f} {baseline['macro_f1']:>12.4f} {baseline['life_f1']:>12.4f}")

if has_lgb:
    print(f"{'  â””â”€ LightGBM (ë‹¨ë…)':<50} {acc_lgb:>12.4f} {f1_lgb:>12.4f} {'-':>12}")
if has_xgb_multi:
    print(f"{'  â””â”€ XGBoost Multi (ë‹¨ë…)':<50} {acc_xgb_multi:>12.4f} {f1_xgb_multi:>12.4f} {'-':>12}")
if has_xgb_optuna:
    print(f"{'  â””â”€ XGBoost Optuna (ë‹¨ë…)':<50} {acc_xgb_optuna:>12.4f} {f1_xgb_optuna:>12.4f} {'-':>12}")

print(f"{'âœ¨ NEW: Weighted Ensemble (Optimized)':<50} {accuracy:>12.4f} {macro_f1:>12.4f} {category_f1[1]:>12.4f}")
print("-"*90)

acc_improve = (accuracy - baseline['accuracy']) * 100
f1_improve = (macro_f1 - baseline['macro_f1']) * 100
life_improve = (category_f1[1] - baseline['life_f1']) * 100

print(f"\nğŸ“Š Baseline ëŒ€ë¹„ ê°œì„ ë„:")
print(f"  Accuracy:    {acc_improve:+.2f}%p")
print(f"  Macro F1:    {f1_improve:+.2f}%p")
print(f"  ìƒí™œ F1:     {life_improve:+.2f}%p")

# ì¢…í•© í‰ê°€
if accuracy > baseline['accuracy'] and macro_f1 > baseline['macro_f1']:
    print(f"\nâœ…âœ…âœ… ì™„ë²½í•œ ì„±ê³µ! ëª¨ë“  ì§€í‘œ ê°œì„ ")
elif accuracy > baseline['accuracy'] or macro_f1 > baseline['macro_f1']:
    print(f"\nâœ…âœ… ìš°ìˆ˜! ì£¼ìš” ì§€í‘œ ê°œì„ ")
else:
    print(f"\nâš ï¸ ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ì— ë¨¸ë¬´ë¦„")

print("\n" + "="*80)
print("âœ… ì•™ìƒë¸” ìµœì í™” ì™„ë£Œ!")
print("="*80)
print(f"\nğŸ“¦ ì €ì¥ íŒŒì¼:")
print(f"   - ë©”íƒ€ë°ì´í„°: {metadata_path}")
print("\n" + "="*80)
