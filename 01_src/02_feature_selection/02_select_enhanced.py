"""
ê°œì„ ëœ í”¼ì²˜(23ê°œ)ë¡œ í”¼ì²˜ ì…€ë ‰ì…˜
ëª©í‘œ: 23ê°œ â†’ 15-18ê°œ í•µì‹¬ í”¼ì²˜ë§Œ ì„ íƒ
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
import json
import os

def main():
    """ë©”ì¸"""
    print("="*70)
    print("í”¼ì²˜ ì…€ë ‰ì…˜ (Enhanced 23ê°œ í”¼ì²˜)")
    print("="*70)
    
    # ë°ì´í„° ë¡œë“œ
    data_file = '02_data/01_processed/preprocessed_enhanced.csv'
    
    if not os.path.exists(data_file):
        print(f"\nâŒ íŒŒì¼ ì—†ìŒ: {data_file}")
        print("ë¨¼ì € ê°œì„ ëœ ì „ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
        print("  python3 01_src/00_preprocessing/03_preprocess_enhanced.py")
        return
    
    print(f"\në°ì´í„° ë¡œë“œ: {data_file}")
    df = pd.read_csv(data_file)
    
    feature_cols = [col for col in df.columns if col.endswith('_scaled')]
    target_col = 'Next_Category_encoded'
    
    X = df[feature_cols].values.astype('float32')
    y = df[target_col].values
    
    print(f"  ì „ì²´ í”¼ì²˜: {len(feature_cols)}ê°œ")
    print(f"  ìƒ˜í”Œ ìˆ˜: {len(X):,}ê°œ")
    
    # ìƒ˜í”Œë§ (CPU ë©”ëª¨ë¦¬ ê³ ë ¤)
    sample_size = min(500000, len(X))
    print(f"\nâš¡ ë¶„ì„ìš© ìƒ˜í”Œ: {sample_size:,}ê°œ")
    
    indices = np.random.choice(len(X), sample_size, replace=False)
    X_sample = X[indices]
    y_sample = y[indices]
    
    # ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X_sample, y_sample, test_size=0.2, random_state=42, stratify=y_sample
    )
    
    # ë² ì´ìŠ¤ë¼ì¸ (ì „ì²´ í”¼ì²˜)
    print("\n" + "="*70)
    print("1ë‹¨ê³„: ë² ì´ìŠ¤ë¼ì¸ (ì „ì²´ í”¼ì²˜)")
    print("="*70)
    print(f"\n{len(feature_cols)}ê°œ í”¼ì²˜ë¡œ í•™ìŠµ ì¤‘...")
    
    model_full = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=8,
        learning_rate=0.1,
        random_state=42,
        n_jobs=-1
    )
    
    model_full.fit(X_train, y_train, verbose=False)
    y_pred = model_full.predict(X_test)
    
    acc_full = accuracy_score(y_test, y_pred)
    f1_full = f1_score(y_test, y_pred, average='macro')
    
    print(f"  Accuracy: {acc_full:.4f}")
    print(f"  Macro F1: {f1_full:.4f}")
    
    # í”¼ì²˜ ì¤‘ìš”ë„
    print("\n" + "="*70)
    print("2ë‹¨ê³„: í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„")
    print("="*70)
    
    importances = model_full.feature_importances_
    importance_df = pd.DataFrame({
        'Feature': [f.replace('_scaled', '') for f in feature_cols],
        'Importance': importances
    }).sort_values('Importance', ascending=False)
    
    print(f"\nìƒìœ„ 20ê°œ í”¼ì²˜:")
    print("="*60)
    for idx, row in importance_df.head(20).iterrows():
        print(f"{row.name+1:2d}. {row['Feature']:40s} {row['Importance']*100:6.2f}%")
    
    # ëˆ„ì  ì¤‘ìš”ë„
    importance_df['Cumsum'] = importance_df['Importance'].cumsum()
    
    # í”¼ì²˜ ì„ íƒ (ëˆ„ì  95%)
    print("\n" + "="*70)
    print("3ë‹¨ê³„: í”¼ì²˜ ì„ íƒ (ëˆ„ì  95% ê¸°ì¤€)")
    print("="*70)
    
    selected_95 = importance_df[importance_df['Cumsum'] <= 0.95]
    
    # ìµœì†Œ 12ê°œ, ìµœëŒ€ 18ê°œ
    if len(selected_95) < 12:
        selected_df = importance_df.head(12)
        print(f"  ëˆ„ì  95% ë¯¸ë§Œ â†’ ìƒìœ„ 12ê°œ ê°•ì œ ì„ íƒ")
    elif len(selected_95) > 18:
        selected_df = importance_df.head(18)
        print(f"  ëˆ„ì  95% ì´ˆê³¼ â†’ ìƒìœ„ 18ê°œë¡œ ì œí•œ")
    else:
        selected_df = selected_95
        print(f"  ëˆ„ì  95% ê¸°ì¤€: {len(selected_df)}ê°œ ì„ íƒ")
    
    selected_features = selected_df['Feature'].tolist()
    selected_features_scaled = [f"{f}_scaled" for f in selected_features]
    
    print(f"\nâœ… ì„ íƒëœ í”¼ì²˜: {len(selected_features)}ê°œ")
    print("="*60)
    for idx, feat in enumerate(selected_features, 1):
        imp = selected_df[selected_df['Feature'] == feat]['Importance'].values[0]
        print(f"{idx:2d}. {feat:40s} {imp*100:6.2f}%")
    
    # ì„ íƒëœ í”¼ì²˜ë¡œ í‰ê°€
    print("\n" + "="*70)
    print("4ë‹¨ê³„: ì„ íƒëœ í”¼ì²˜ ì„±ëŠ¥ í‰ê°€")
    print("="*70)
    
    selected_indices = [feature_cols.index(f) for f in selected_features_scaled]
    X_train_selected = X_train[:, selected_indices]
    X_test_selected = X_test[:, selected_indices]
    
    model_selected = xgb.XGBClassifier(
        n_estimators=150,
        max_depth=10,
        learning_rate=0.1,
        random_state=42,
        n_jobs=-1
    )
    
    model_selected.fit(X_train_selected, y_train, verbose=False)
    y_pred_selected = model_selected.predict(X_test_selected)
    
    acc_selected = accuracy_score(y_test, y_pred_selected)
    f1_selected = f1_score(y_test, y_pred_selected, average='macro')
    
    print(f"\nì „ì²´ í”¼ì²˜ ({len(feature_cols)}ê°œ):")
    print(f"  Accuracy: {acc_full:.4f}")
    print(f"  Macro F1: {f1_full:.4f}")
    
    print(f"\nì„ íƒëœ í”¼ì²˜ ({len(selected_features)}ê°œ):")
    print(f"  Accuracy: {acc_selected:.4f} ({acc_selected-acc_full:+.4f})")
    print(f"  Macro F1: {f1_selected:.4f} ({f1_selected-f1_full:+.4f})")
    print(f"  ë©”ëª¨ë¦¬ ì ˆê°: {(1-len(selected_features)/len(feature_cols))*100:.1f}%")
    print(f"  ì†ë„ í–¥ìƒ: ì˜ˆìƒ ~{(1-len(selected_features)/len(feature_cols))*100:.0f}%")
    
    # ì €ì¥
    selected_info = {
        'selected_features': selected_features,
        'num_features': len(selected_features),
        'total_features': len(feature_cols),
        'reduction': f"{(1-len(selected_features)/len(feature_cols))*100:.1f}%",
        'performance': {
            'baseline_accuracy': float(acc_full),
            'baseline_f1': float(f1_full),
            'selected_accuracy': float(acc_selected),
            'selected_f1': float(f1_selected),
            'accuracy_diff': float(acc_selected - acc_full),
            'f1_diff': float(f1_selected - f1_full)
        }
    }
    
    output_file = '02_data/01_processed/selected_features_enhanced.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(selected_info, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ì €ì¥: {output_file}")
    
    # ì œê±°ëœ í”¼ì²˜ í™•ì¸
    removed_features = set([f.replace('_scaled', '') for f in feature_cols]) - set(selected_features)
    if removed_features:
        print(f"\nì œê±°ëœ í”¼ì²˜ ({len(removed_features)}ê°œ):")
        for feat in sorted(removed_features):
            imp = importance_df[importance_df['Feature'] == feat]['Importance'].values
            if len(imp) > 0:
                print(f"  - {feat:40s} {imp[0]*100:6.2f}%")
    
    print("\n" + "="*70)
    print("í”¼ì²˜ ì…€ë ‰ì…˜ ì™„ë£Œ!")
    print("="*70)
    print(f"\nğŸ’¡ ê¶Œì¥: {len(selected_features)}ê°œ í”¼ì²˜ ì‚¬ìš©")
    print(f"   ({len(feature_cols)} â†’ {len(selected_features)}ê°œ)")
    print(f"   ë©”ëª¨ë¦¬: -{(1-len(selected_features)/len(feature_cols))*100:.0f}%, ì„±ëŠ¥ ì†ì‹¤: {(f1_selected-f1_full)*100:+.2f}%p")


if __name__ == '__main__':
    main()
