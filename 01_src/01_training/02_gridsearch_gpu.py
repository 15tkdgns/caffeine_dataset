"""
GPU ê¸°ë°˜ ê·¸ë¦¬ë“œ ì„œì¹˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸
ì „ì²´ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score, f1_score
import joblib
import os
import sys
from datetime import datetime
import json

# cuML ì„í¬íŠ¸ ì‹œë„
try:
    from cuml.ensemble import RandomForestClassifier as CumlRFClassifier
    HAS_CUML = True
except ImportError:
    HAS_CUML = False
    print("ê²½ê³ : cuMLì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. XGBoostë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")


def load_preprocessed_data(file_path, sample_frac=1.0):
    """
    ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
    
    Args:
        file_path: ì „ì²˜ë¦¬ëœ CSV íŒŒì¼ ê²½ë¡œ
        sample_frac: ì‚¬ìš©í•  ë°ì´í„° ë¹„ìœ¨ (0.0-1.0), í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŒ
    """
    print(f"\në°ì´í„° ë¡œë“œ ì¤‘: {file_path}")
    df = pd.read_csv(file_path)
    
    if sample_frac < 1.0:
        print(f"  - ìƒ˜í”Œë§: {sample_frac*100}% ì‚¬ìš©")
        df = df.sample(frac=sample_frac, random_state=42)
    
    print(f"  - ì´ ìƒ˜í”Œ ìˆ˜: {len(df):,}ê±´")
    print(f"  - ì´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ")
    
    return df


def prepare_features_target(df):
    """íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬"""
    # '_scaled'ë¡œ ëë‚˜ëŠ” ì»¬ëŸ¼ì´ íŠ¹ì„±
    feature_cols = [col for col in df.columns if col.endswith('_scaled')]
    target_col = 'Category_encoded'
    
    X = df[feature_cols].values
    y = df[target_col].values
    
    print(f"\níŠ¹ì„± ë° íƒ€ê²Ÿ ì¤€ë¹„:")
    print(f"  - íŠ¹ì„± ìˆ˜: {len(feature_cols)}ê°œ")
    print(f"  - ìƒ˜í”Œ ìˆ˜: {len(X):,}ê°œ")
    print(f"  - í´ë˜ìŠ¤ ìˆ˜: {len(np.unique(y))}ê°œ")
    
    return X, y, feature_cols


def xgboost_grid_search_gpu(X_train, y_train, X_test, y_test):
    """
    XGBoost GPU ê·¸ë¦¬ë“œ ì„œì¹˜
    """
    print("\n" + "="*70)
    print("XGBoost GPU ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹œì‘")
    print("="*70)
    
    # ê·¸ë¦¬ë“œ ì„œì¹˜ íŒŒë¼ë¯¸í„°
    param_grid = {
        'max_depth': [6, 8, 10],
        'learning_rate': [0.01, 0.05, 0.1],
        'n_estimators': [100, 300, 500],
        'subsample': [0.8, 1.0],
        'colsample_bytree': [0.8, 1.0],
    }
    
    # ë² ì´ìŠ¤ ëª¨ë¸ (GPU ì‚¬ìš©)
    base_model = xgb.XGBClassifier(
        device='cuda',
        tree_method='hist',
        random_state=42,
        eval_metric='mlogloss'
    )
    
    print(f"ê·¸ë¦¬ë“œ ì„œì¹˜ ì„¤ì •:")
    print(f"  - íƒìƒ‰ ì¡°í•© ìˆ˜: {np.prod([len(v) for v in param_grid.values()])}ê°œ")
    print(f"  - CV Folds: 3")
    
    # ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰
    grid_search = GridSearchCV(
        estimator=base_model,
        param_grid=param_grid,
        cv=3,
        scoring='f1_weighted',
        n_jobs=1,  # GPU ì‚¬ìš© ì‹œ 1ë¡œ ì„¤ì •
        verbose=2,
        return_train_score=True
    )
    
    start_time = datetime.now()
    print(f"\ní•™ìŠµ ì‹œì‘: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    grid_search.fit(X_train, y_train)
    
    training_time = (datetime.now() - start_time).total_seconds()
    print(f"í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {training_time:.2f}ì´ˆ ({training_time/60:.2f}ë¶„)")
    
    # ìµœì  íŒŒë¼ë¯¸í„°
    print(f"\nìµœì  íŒŒë¼ë¯¸í„°:")
    for param, value in grid_search.best_params_.items():
        print(f"  - {param}: {value}")
    
    print(f"\nìµœì  CV ì ìˆ˜: {grid_search.best_score_:.4f}")
    
    # í…ŒìŠ¤íŠ¸ í‰ê°€
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred)
    test_f1 = f1_score(y_test, y_pred, average='weighted')
    
    print(f"\ní…ŒìŠ¤íŠ¸ ì„±ëŠ¥:")
    print(f"  - Accuracy: {test_accuracy:.4f}")
    print(f"  - F1 Score (weighted): {test_f1:.4f}")
    
    return grid_search, {
        'model_name': 'xgboost',
        'best_params': grid_search.best_params_,
        'best_cv_score': grid_search.best_score_,
        'test_accuracy': test_accuracy,
        'test_f1': test_f1,
        'training_time': training_time
    }


def randomforest_grid_search_gpu(X_train, y_train, X_test, y_test):
    """
    cuML RandomForest GPU ê·¸ë¦¬ë“œ ì„œì¹˜
    """
    if not HAS_CUML:
        print("\ncuMLì´ ì—†ì–´ì„œ RandomForest ê·¸ë¦¬ë“œ ì„œì¹˜ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None, None
    
    print("\n" + "="*70)
    print("cuML RandomForest GPU ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹œì‘")
    print("="*70)
    
    # íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ (cuMLìš©)
    param_grid = {
        'n_estimators': [100, 300, 500],
        'max_depth': [10, 16, 20],
        'max_features': [0.5, 0.8, 1.0],
    }
    
    base_model = CumlRFClassifier(
        random_state=42,
        n_streams=1
    )
    
    print(f"ê·¸ë¦¬ë“œ ì„œì¹˜ ì„¤ì •:")
    print(f"  - íƒìƒ‰ ì¡°í•© ìˆ˜: {np.prod([len(v) for v in param_grid.values()])}ê°œ")
    print(f"  - CV Folds: 3")
    
    grid_search = GridSearchCV(
        estimator=base_model,
        param_grid=param_grid,
        cv=3,
        scoring='f1_weighted',
        n_jobs=1,
        verbose=2
    )
    
    start_time = datetime.now()
    print(f"\ní•™ìŠµ ì‹œì‘: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    grid_search.fit(X_train, y_train)
    
    training_time = (datetime.now() - start_time).total_seconds()
    print(f"í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {training_time:.2f}ì´ˆ ({training_time/60:.2f}ë¶„)")
    
    # ìµœì  íŒŒë¼ë¯¸í„°
    print(f"\nìµœì  íŒŒë¼ë¯¸í„°:")
    for param, value in grid_search.best_params_.items():
        print(f"  - {param}: {value}")
    
    print(f"\nìµœì  CV ì ìˆ˜: {grid_search.best_score_:.4f}")
    
    # í…ŒìŠ¤íŠ¸ í‰ê°€
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred)
    test_f1 = f1_score(y_test, y_pred, average='weighted')
    
    print(f"\ní…ŒìŠ¤íŠ¸ ì„±ëŠ¥:")
    print(f"  - Accuracy: {test_accuracy:.4f}")
    print(f"  - F1 Score (weighted): {test_f1:.4f}")
    
    return grid_search, {
        'model_name': 'randomforest_cuml',
        'best_params': grid_search.best_params_,
        'best_cv_score': grid_search.best_score_,
        'test_accuracy': test_accuracy,
        'test_f1': test_f1,
        'training_time': training_time
    }


def save_results(grid_search, metadata, model_name, output_dir='03_models/05_gridsearch'):
    """ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ì €ì¥"""
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ìµœì  ëª¨ë¸ ì €ì¥
    model_file = os.path.join(output_dir, f'best_{model_name}_{timestamp}.joblib')
    joblib.dump(grid_search.best_estimator_, model_file)
    print(f"\nìµœì  ëª¨ë¸ ì €ì¥: {model_file}")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata_file = os.path.join(output_dir, f'metadata_{model_name}_{timestamp}.json')
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)
    print(f"ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_file}")
    
    # CV ê²°ê³¼ ì €ì¥
    cv_results = pd.DataFrame(grid_search.cv_results_)
    cv_file = os.path.join(output_dir, f'cv_results_{model_name}_{timestamp}.csv')
    cv_results.to_csv(cv_file, index=False)
    print(f"CV ê²°ê³¼ ì €ì¥: {cv_file}")
    
    return model_file, metadata_file, cv_file


def main():
    """ë©”ì¸ íŒŒì´í”„ë¼ì¸"""
    print("="*70)
    print("GPU ê¸°ë°˜ ì „ì²´ ë°ì´í„° ê·¸ë¦¬ë“œ ì„œì¹˜ íŒŒì´í”„ë¼ì¸")
    print("="*70)
    
    # ì„¤ì •
    data_file = '02_data/01_processed/preprocessed_full_featured.csv'
    sample_frac = 1.0  # 1.0 = ì „ì²´ ë°ì´í„°, í…ŒìŠ¤íŠ¸ ì‹œ 0.1 ë“±ìœ¼ë¡œ ì¡°ì ˆ
    test_size = 0.2
    
    # 1. ë°ì´í„° ë¡œë“œ
    df = load_preprocessed_data(data_file, sample_frac=sample_frac)
    
    # 2. íŠ¹ì„±/íƒ€ê²Ÿ ì¤€ë¹„
    X, y, feature_cols = prepare_features_target(df)
    
    # 3. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
    print(f"\ní•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬ (test_size={test_size})...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=y
    )
    print(f"  - í•™ìŠµ ë°ì´í„°: {len(X_train):,}ê±´")
    print(f"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test):,}ê±´")
    
    # ê²°ê³¼ ì €ì¥ìš©
    all_results = []
    
    # 4. XGBoost ê·¸ë¦¬ë“œ ì„œì¹˜
    try:
        xgb_grid, xgb_metadata = xgboost_grid_search_gpu(X_train, y_train, X_test, y_test)
        if xgb_grid:
            save_results(xgb_grid, xgb_metadata, 'xgboost')
            all_results.append(xgb_metadata)
    except Exception as e:
        print(f"\nXGBoost ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤íŒ¨: {e}")
    
    # 5. RandomForest ê·¸ë¦¬ë“œ ì„œì¹˜
    try:
        rf_grid, rf_metadata = randomforest_grid_search_gpu(X_train, y_train, X_test, y_test)
        if rf_grid:
            save_results(rf_grid, rf_metadata, 'randomforest')
            all_results.append(rf_metadata)
    except Exception as e:
        print(f"\nRandomForest ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤íŒ¨: {e}")
    
    # 6. ìµœì¢… ìš”ì•½
    print("\n" + "="*70)
    print("ê·¸ë¦¬ë“œ ì„œì¹˜ ì™„ë£Œ - ìµœì¢… ìš”ì•½")
    print("="*70)
    
    if all_results:
        summary_df = pd.DataFrame(all_results)
        print("\nëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ:")
        print(summary_df[['model_name', 'test_accuracy', 'test_f1', 'training_time']])
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_idx = summary_df['test_f1'].idxmax()
        best_model = summary_df.loc[best_idx]
        
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['model_name']}")
        print(f"  - Test Accuracy: {best_model['test_accuracy']:.4f}")
        print(f"  - Test F1: {best_model['test_f1']:.4f}")
        print(f"  - í•™ìŠµ ì‹œê°„: {best_model['training_time']:.2f}ì´ˆ")
    else:
        print("\nì™„ë£Œëœ ê·¸ë¦¬ë“œ ì„œì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    print("\n" + "="*70)
    print("íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print("="*70)


if __name__ == '__main__':
    main()
