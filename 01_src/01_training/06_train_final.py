"""
ì„ íƒëœ 16ê°œ í•µì‹¬ í”¼ì²˜ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
- Refer í”¼ì²˜ í¬í•¨
- ë¶ˆê· í˜• ë³´ì •
- GPU ê°€ì†
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score
from sklearn.utils.class_weight import compute_sample_weight
import joblib
import json
import os
from datetime import datetime

# ì„ íƒëœ í”¼ì²˜ ë¡œë“œ
def load_selected_features():
    """í”¼ì²˜ ì…€ë ‰ì…˜ ê²°ê³¼ ë¡œë“œ"""
    feature_file = '02_data/01_processed/selected_features_enhanced.json'
    with open(feature_file, 'r', encoding='utf-8') as f:
        feature_info = json.load(f)
    
    selected_features = [f"{f}_scaled" for f in feature_info['selected_features']]
    
    print(f"ì„ íƒëœ í”¼ì²˜: {len(selected_features)}ê°œ")
    for i, feat in enumerate(feature_info['selected_features'], 1):
        print(f"  {i:2d}. {feat}")
    
    return selected_features


def load_data(file_path, selected_features):
    """ë°ì´í„° ë¡œë“œ (ì„ íƒëœ í”¼ì²˜ë§Œ)"""
    print(f"\në°ì´í„° ë¡œë“œ: {file_path}")
    df = pd.read_csv(file_path)
    
    # ì„ íƒëœ í”¼ì²˜ë§Œ ì¶”ì¶œ
    X = df[selected_features].values.astype('float32')
    y = df['Next_Category_encoded'].values
    
    print(f"  ìƒ˜í”Œ: {len(X):,}ê°œ")
    print(f"  í”¼ì²˜: {len(selected_features)}ê°œ")
    print(f"  ë©”ëª¨ë¦¬: {X.nbytes / 1024**2:.1f} MB")
    
    return X, y


def train_final_model(X_train, y_train, X_test, y_test, sample_weights_train):
    """ìµœì¢… ëª¨ë¸ í•™ìŠµ"""
    print("\n" + "="*70)
    print("ìµœì¢… ëª¨ë¸ í•™ìŠµ (XGBoost GPU)")
    print("="*70)
    
    # ìµœì  íŒŒë¼ë¯¸í„° (ê·¸ë¦¬ë“œ ì„œì¹˜ + ì¶”ê°€ íŠœë‹)
    best_params = {
        'max_depth': 10,
        'learning_rate': 0.1,
        'n_estimators': 300,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'min_child_weight': 3,
        'gamma': 0.1,
    }
    
    print(f"í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    for k, v in best_params.items():
        print(f"  {k}: {v}")
    
    model = xgb.XGBClassifier(
        device='cuda',
        tree_method='hist',
        random_state=42,
        eval_metric='mlogloss',
        **best_params
    )
    
    print(f"\ní•™ìŠµ ì‹œì‘...")
    start_time = datetime.now()
    
    # ë¶ˆê· í˜• ë³´ì • ì ìš©
    model.fit(X_train, y_train, sample_weight=sample_weights_train, verbose=False)
    
    training_time = (datetime.now() - start_time).total_seconds()
    print(f"í•™ìŠµ ì™„ë£Œ: {training_time:.1f}ì´ˆ ({training_time/60:.1f}ë¶„)")
    
    return model, training_time


def evaluate_model(model, X_test, y_test):
    """ëª¨ë¸ í‰ê°€"""
    print("\n" + "="*70)
    print("ëª¨ë¸ í‰ê°€")
    print("="*70)
    
    y_pred = model.predict(X_test)
    
    acc = accuracy_score(y_test, y_pred)
    f1_macro = f1_score(y_test, y_pred, average='macro')
    f1_weighted = f1_score(y_test, y_pred, average='weighted')
    
    print(f"\nì „ì²´ ì„±ëŠ¥:")
    print(f"  Accuracy:      {acc:.4f} ({acc*100:.2f}%)")
    print(f"  Macro F1:      {f1_macro:.4f}")
    print(f"  Weighted F1:   {f1_weighted:.4f}")
    
    # Refer ëª¨ë¸ê³¼ ë¹„êµ
    refer_acc = 0.6309
    refer_f1 = 0.5486
    
    print(f"\nRefer ëª¨ë¸ ëŒ€ë¹„:")
    print(f"  Accuracy:  {acc:.4f} vs {refer_acc:.4f} ({(acc-refer_acc)*100:+.2f}%p)")
    print(f"  Macro F1:  {f1_macro:.4f} vs {refer_f1:.4f} ({(f1_macro-refer_f1)*100:+.2f}%p)")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥
    categories = ['êµí†µ', 'ìƒí™œ', 'ì‡¼í•‘', 'ì‹ë£Œí’ˆ', 'ì™¸ì‹', 'ì£¼ìœ ']
    print(f"\nì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥:")
    print(classification_report(y_test, y_pred, target_names=categories, digits=4))
    
    return {
        'accuracy': acc,
        'macro_f1': f1_macro,
        'weighted_f1': f1_weighted
    }


def save_model(model, metadata, output_dir='03_models/08_final'):
    """ìµœì¢… ëª¨ë¸ ì €ì¥"""
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ëª¨ë¸ ì €ì¥
    model_file = os.path.join(output_dir, f'final_model_{timestamp}.joblib')
    joblib.dump(model, model_file)
    print(f"\nâœ… ëª¨ë¸ ì €ì¥: {model_file}")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata_file = os.path.join(output_dir, f'metadata_{timestamp}.json')
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"âœ… ë©”íƒ€ë°ì´í„°: {metadata_file}")
    
    return model_file, metadata_file


def main():
    """ë©”ì¸"""
    print("="*70)
    print("ìµœì¢… ëª¨ë¸ í•™ìŠµ (ì„ íƒëœ 16ê°œ í”¼ì²˜)")
    print("="*70)
    
    # 1. ì„ íƒëœ í”¼ì²˜ ë¡œë“œ
    selected_features = load_selected_features()
    
    # 2. ë°ì´í„° ë¡œë“œ
    data_file = '02_data/01_processed/preprocessed_enhanced.csv'
    X, y = load_data(data_file, selected_features)
    
    # 3. ë¶ˆê· í˜• ë³´ì •
    print("\në¶ˆê· í˜• ë³´ì • ì ìš©...")
    sample_weights = compute_sample_weight('balanced', y)
    unique_classes, class_counts = np.unique(y, return_counts=True)
    print(f"í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜:")
    categories = ['êµí†µ', 'ìƒí™œ', 'ì‡¼í•‘', 'ì‹ë£Œí’ˆ', 'ì™¸ì‹', 'ì£¼ìœ ']
    for cls, count, cat in zip(unique_classes, class_counts, categories):
        weight = sample_weights[y == cls][0]
        print(f"  {cat:6s}: {count:,}ê±´ â†’ ê°€ì¤‘ì¹˜ {weight:.3f}")
    
    # 4. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
    print(f"\ní•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í•  (80:20)...")
    X_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(
        X, y, sample_weights, test_size=0.2, random_state=42, stratify=y
    )
    print(f"  í•™ìŠµ: {len(X_train):,}ê°œ")
    print(f"  í…ŒìŠ¤íŠ¸: {len(X_test):,}ê°œ")
    
    # 5. ëª¨ë¸ í•™ìŠµ
    model, training_time = train_final_model(X_train, y_train, X_test, y_test, sw_train)
    
    # 6. ëª¨ë¸ í‰ê°€
    performance = evaluate_model(model, X_test, y_test)
    
    # 7. ëª¨ë¸ ì €ì¥
    metadata = {
        'model_name': 'xgboost_final_16features',
        'model_version': 'v1.0',
        'num_features': len(selected_features),
        'selected_features': [f.replace('_scaled', '') for f in selected_features],
        'training_samples': len(X_train),
        'test_samples': len(X_test),
        'training_time_seconds': training_time,
        'performance': performance,
        'refer_comparison': {
            'refer_accuracy': 0.6309,
            'refer_macro_f1': 0.5486,
            'accuracy_gap': performance['accuracy'] - 0.6309,
            'f1_gap': performance['macro_f1'] - 0.5486
        },
        'enhancements': [
            'Refer í”¼ì²˜ 6ê°œ (User_*_Ratio)',
            'ë¶ˆê· í˜• ë³´ì • (sample_weight)',
            'GPU ê°€ì† (CUDA)',
            'í”¼ì²˜ ì…€ë ‰ì…˜ (27ê°œ â†’ 16ê°œ)',
            'Macro F1 ìµœì í™”'
        ],
        'created_at': datetime.now().isoformat()
    }
    
    save_model(model, metadata)
    
    # 8. ìµœì¢… ìš”ì•½
    print("\n" + "="*70)
    print("ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("="*70)
    print(f"\nğŸ“Š ì„±ëŠ¥:")
    print(f"  Accuracy:    {performance['accuracy']:.4f} ({performance['accuracy']*100:.2f}%)")
    print(f"  Macro F1:    {performance['macro_f1']:.4f}")
    print(f"  Weighted F1: {performance['weighted_f1']:.4f}")
    
    print(f"\nğŸ“ˆ Refer ëª¨ë¸ ëŒ€ë¹„:")
    print(f"  Accuracy ê°­: {(performance['accuracy']-0.6309)*100:+.2f}%p")
    print(f"  Macro F1 ê°­: {(performance['macro_f1']-0.5486)*100:+.2f}%p")
    
    print(f"\nâš¡ íš¨ìœ¨:")
    print(f"  í”¼ì²˜ ìˆ˜: 27ê°œ â†’ 16ê°œ (40% ì ˆê°)")
    print(f"  í•™ìŠµ ì‹œê°„: {training_time:.1f}ì´ˆ")
    print(f"  ë©”ëª¨ë¦¬: {X.nbytes / 1024**2:.1f} MB")
    
    print(f"\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"  1. ì´ ëª¨ë¸ë¡œ FastAPI ì„œë¹„ìŠ¤ êµ¬í˜„")
    print(f"  2. Streamlit ëŒ€ì‹œë³´ë“œ ì—°ë™")
    print(f"  3. ì¶”ê°€ ì„±ëŠ¥ ê°œì„  (ì•™ìƒë¸”, íŒŒì¸íŠœë‹)")
    
    print("\n" + "="*70)
    print("í•™ìŠµ ì™„ë£Œ!")
    print("="*70)


if __name__ == '__main__':
    main()
