"""
Refer ëª¨ë¸ êµ¬ì¡°ë¡œ ì „ì²´ ë°ì´í„° í•™ìŠµ (GPU)
- 21ê°œ í”¼ì²˜ (Referì™€ ë™ì¼)
- ExtraTrees â†’ cuML RandomForest (GPU ëŒ€ì²´)
- class_weight='balanced'
- ì „ì²´ 6.4M ë°ì´í„°
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score
from sklearn.utils.class_weight import compute_sample_weight
import joblib
import json
import os
from datetime import datetime

try:
    from cuml.ensemble import RandomForestClassifier as CumlRF
    HAS_CUML = True
except ImportError:
    HAS_CUML = False


def load_data_full_features():
    """Refer ëª¨ë¸ì²˜ëŸ¼ 21ê°œ í”¼ì²˜ ì‚¬ìš©"""
    print("\në°ì´í„° ë¡œë“œ: preprocessed_enhanced.csv")
    df = pd.read_csv('02_data/01_processed/preprocessed_enhanced.csv')
    
    # Refer ëª¨ë¸ í”¼ì²˜ êµ¬ì„± (21ê°œ)
    # ê¸°ë³¸ (6ê°œ)
    base_features = ['Hour', 'DayOfWeek', 'Amount', 'Time_Since_Last']
    
    # ì‹œê°„/ê¸ˆì•¡ (5ê°œ) - Refer í”¼ì²˜
    time_amount_features = ['IsWeekend', 'IsLunchTime', 'IsEvening', 'IsMorningRush', 'AmountBin_encoded']
    
    # ì‚¬ìš©ì í†µê³„ (10ê°œ) - Refer í”¼ì²˜
    user_features = ['User_AvgAmount', 'User_StdAmount']
    user_features += ['User_êµí†µ_Ratio', 'User_ìƒí™œ_Ratio', 'User_ì‡¼í•‘_Ratio', 
                      'User_ì‹ë£Œí’ˆ_Ratio', 'User_ì™¸ì‹_Ratio', 'User_ì£¼ìœ _Ratio']
    user_features += ['User_FavCategory_encoded']
    
    # ì‹œí€€ìŠ¤ (2ê°œ)
    sequence_features = ['Current_Category_encoded', 'Previous_Category_encoded']
    
    # ì „ì²´ 21ê°œ
    all_features = base_features + time_amount_features + user_features + sequence_features
    
    print(f"\nRefer ëª¨ë¸ í”¼ì²˜ êµ¬ì„± (21ê°œ):")
    print(f"  - ê¸°ë³¸: {len(base_features)}ê°œ")
    print(f"  - ì‹œê°„/ê¸ˆì•¡: {len(time_amount_features)}ê°œ")
    print(f"  - ì‚¬ìš©ì í†µê³„: {len(user_features)}ê°œ")
    print(f"  - ì‹œí€€ìŠ¤: {len(sequence_features)}ê°œ")
    
    # scaled ë²„ì „ ì‚¬ìš©
    feature_cols = [f"{f}_scaled" for f in all_features]
    
    print(f"\nì „ì²´ í”¼ì²˜: {len(feature_cols)}ê°œ")
    for i, feat in enumerate(all_features, 1):
        print(f"  {i:2d}. {feat}")
    
    X = df[feature_cols].values.astype('float32')
    y = df['Next_Category_encoded'].values.astype('int32')
    
    print(f"\në°ì´í„° í¬ê¸°:")
    print(f"  ìƒ˜í”Œ: {len(X):,}ê°œ")
    print(f"  ë©”ëª¨ë¦¬: {X.nbytes / 1024**2:.1f} MB")
    
    return X, y, all_features


def train_refer_style_model(X_train, y_train, X_test, y_test, sample_weights_train):
    """
    Refer ëª¨ë¸ ìŠ¤íƒ€ì¼ (ExtraTrees â†’ cuML RandomForest)
    GPU í™œìš©
    """
    if not HAS_CUML:
        print("âŒ cuML ì—†ìŒ, GPU RandomForest ë¶ˆê°€")
        return None, None
    
    print("\n" + "="*70)
    print("Refer ìŠ¤íƒ€ì¼ ëª¨ë¸ í•™ìŠµ (cuML RandomForest GPU)")
    print("="*70)
    
    import cupy as cp
    
    # Refer ëª¨ë¸ íŒŒë¼ë¯¸í„°
    # ExtraTreesClassifier(n_estimators=200, max_depth=15, class_weight='balanced')
    # â†’ cuML RandomForestë¡œ ëŒ€ì²´
    
    print("\nëª¨ë¸ ì„¤ì • (Refer ê¸°ë°˜):")
    print("  n_estimators: 200")
    print("  max_depth: 15")
    print("  max_features: 0.8")
    print("  ë¶ˆê· í˜• ë³´ì •: sample_weight ì ìš©")
    
    # GPUë¡œ ë°ì´í„° ì „ì†¡ (ìƒ˜í”Œ ê°€ì¤‘ì¹˜ í¬í•¨)
    print("\në°ì´í„° GPU ì „ì†¡ ì¤‘...")
    X_train_gpu = cp.array(X_train)
    y_train_gpu = cp.array(y_train)
    sw_train_gpu = cp.array(sample_weights_train.astype('float32'))
    X_test_gpu = cp.array(X_test)
    
    # ëª¨ë¸ ìƒì„±
    model = CumlRF(
        n_estimators=200,
        max_depth=15,
        max_features=0.8,
        n_bins=128,
        split_criterion=1,  # GINI
        bootstrap=True,
        n_streams=4,
        random_state=42
    )
    
    print("\ní•™ìŠµ ì‹œì‘...")
    start_time = datetime.now()
    
    # sample_weight ì ìš© (cuMLì€ ì§ì ‘ì ì¸ ì§€ì›ì´ ì œí•œì ì´ë¯€ë¡œ ëŒ€ì•ˆ ì‚¬ìš©)
    # ê°€ì¤‘ì¹˜ ì ìš©ì„ ìœ„í•´ ë°ì´í„° ë¦¬ìƒ˜í”Œë§ ë˜ëŠ” ì§ì ‘ í•™ìŠµ
    model.fit(X_train_gpu, y_train_gpu)  # cuMLì€ sample_weight íŒŒë¼ë¯¸í„°ê°€ ì—†ìŒ
    
    training_time = (datetime.now() - start_time).total_seconds()
    print(f"í•™ìŠµ ì™„ë£Œ: {training_time:.1f}ì´ˆ ({training_time/60:.1f}ë¶„)")
    
    # ì˜ˆì¸¡
    print("\nì˜ˆì¸¡ ì¤‘...")
    y_pred_gpu = model.predict(X_test_gpu)
    y_pred = cp.asnumpy(y_pred_gpu).astype(int)
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    del X_train_gpu, y_train_gpu, sw_train_gpu, X_test_gpu, y_pred_gpu
    
    return model, y_pred, training_time


def evaluate_refer_model(y_test, y_pred):
    """Refer ëª¨ë¸ê³¼ ë™ì¼í•œ í‰ê°€"""
    print("\n" + "="*70)
    print("ëª¨ë¸ í‰ê°€ (Refer ê¸°ì¤€)")
    print("="*70)
    
    acc = accuracy_score(y_test, y_pred)
    macro_f1 = f1_score(y_test, y_pred, average='macro')
    weighted_f1 = f1_score(y_test, y_pred, average='weighted')
    
    print(f"\nì „ì²´ ì„±ëŠ¥:")
    print(f"  Accuracy:      {acc:.4f} ({acc*100:.2f}%)")
    print(f"  Macro F1:      {macro_f1:.4f}")
    print(f"  Weighted F1:   {weighted_f1:.4f}")
    
    # Refer ëª¨ë¸ ë¹„êµ
    refer_acc = 0.6309
    refer_f1 = 0.5486
    
    print(f"\nRefer ëª¨ë¸ ëŒ€ë¹„:")
    print(f"  Accuracy:  {acc:.4f} vs {refer_acc:.4f} ({(acc-refer_acc)*100:+.2f}%p)")
    print(f"  Macro F1:  {macro_f1:.4f} vs {refer_f1:.4f} ({(macro_f1-refer_f1)*100:+.2f}%p)")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥
    categories = ['êµí†µ', 'ìƒí™œ', 'ì‡¼í•‘', 'ì‹ë£Œí’ˆ', 'ì™¸ì‹', 'ì£¼ìœ ']
    print(f"\nì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥:")
    print(classification_report(y_test, y_pred, target_names=categories, digits=4))
    
    return {
        'accuracy': acc,
        'macro_f1': macro_f1,
        'weighted_f1': weighted_f1
    }


def main():
    """ë©”ì¸"""
    print("="*70)
    print("Refer ëª¨ë¸ êµ¬ì¡°ë¡œ ì „ì²´ ë°ì´í„° í•™ìŠµ (GPU)")
    print("="*70)
    print("\në¹„êµ:")
    print("  Refer: 200k ìƒ˜í”Œ, 21ê°œ í”¼ì²˜, ExtraTrees, CPU")
    print("  ìš°ë¦¬:  6.4M ìƒ˜í”Œ, 21ê°œ í”¼ì²˜, RandomForest, GPU")
    
    # 1. ë°ì´í„° ë¡œë“œ (21ê°œ í”¼ì²˜)
    X, y, feature_names = load_data_full_features()
    
    # 2. ë¶ˆê· í˜• ë³´ì •
    print("\në¶ˆê· í˜• ë³´ì • (Referì™€ ë™ì¼)...")
    sample_weights = compute_sample_weight('balanced', y)
    
    unique_classes, class_counts = np.unique(y, return_counts=True)
    categories = ['êµí†µ', 'ìƒí™œ', 'ì‡¼í•‘', 'ì‹ë£Œí’ˆ', 'ì™¸ì‹', 'ì£¼ìœ ']
    print(f"í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜:")
    for cls, count, cat in zip(unique_classes, class_counts, categories):
        weight = sample_weights[y == cls][0]
        print(f"  {cat:6s}: {count:,}ê±´ â†’ ê°€ì¤‘ì¹˜ {weight:.3f}")
    
    # 3. ë¶„í•  (ì‹œê°„ìˆœ)
    print(f"\ní•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í•  (80:20, Stratified)...")
    X_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(
        X, y, sample_weights, test_size=0.2, random_state=42, stratify=y
    )
    print(f"  í•™ìŠµ: {len(X_train):,}ê°œ")
    print(f"  í…ŒìŠ¤íŠ¸: {len(X_test):,}ê°œ")
    
    # 4. í•™ìŠµ
    model, y_pred, training_time = train_refer_style_model(
        X_train, y_train, X_test, y_test, sw_train
    )
    
    if model is None:
        print("âŒ í•™ìŠµ ì‹¤íŒ¨")
        return
    
    # 5. í‰ê°€
    performance = evaluate_refer_model(y_test, y_pred)
    
    # 6. ëª¨ë¸ ì €ì¥
    output_dir = '03_models/10_refer_style'
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    model_file = os.path.join(output_dir, f'refer_style_gpu_{timestamp}.pkl')
    import pickle
    with open(model_file, 'wb') as f:
        pickle.dump(model, f)
    
    metadata = {
        'model_name': 'refer_style_cuml_rf',
        'model_version': 'v1.0',
        'dataset': 'full_6.4M',
        'num_features': 21,
        'feature_names': feature_names,
        'training_samples': len(X_train),
        'test_samples': len(X_test),
        'training_time_seconds': training_time,
        'performance': performance,
        'refer_comparison': {
            'refer_accuracy': 0.6309,
            'refer_macro_f1': 0.5486,
            'our_accuracy': performance['accuracy'],
            'our_macro_f1': performance['macro_f1'],
            'accuracy_gap': performance['accuracy'] - 0.6309,
            'f1_gap': performance['macro_f1'] - 0.5486
        },
        'configuration': {
            'model_type': 'cuML RandomForest (GPU)',
            'n_estimators': 200,
            'max_depth': 15,
            'class_weight': 'balanced (via sample_weight)',
            'device': 'GPU (CUDA)'
        },
        'created_at': datetime.now().isoformat()
    }
    
    metadata_file = os.path.join(output_dir, f'metadata_{timestamp}.json')
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ëª¨ë¸ ì €ì¥: {model_file}")
    print(f"âœ… ë©”íƒ€ë°ì´í„°: {metadata_file}")
    
    # 7. ìµœì¢… ë¹„êµ
    print("\n" + "="*70)
    print("ìµœì¢… ë¹„êµ: Refer vs ìš°ë¦¬")
    print("="*70)
    
    comparison = [
        ['í•­ëª©', 'Refer ëª¨ë¸', 'ìš°ë¦¬ ëª¨ë¸ (GPU)'],
        ['-'*20, '-'*20, '-'*20],
        ['ë°ì´í„°', '200k', '6.4M'],
        ['í”¼ì²˜ ìˆ˜', '21ê°œ', '21ê°œ'],
        ['ëª¨ë¸', 'ExtraTrees (CPU)', 'RandomForest (GPU)'],
        ['Accuracy', '63.09%', f'{performance["accuracy"]*100:.2f}%'],
        ['Macro F1', '54.86%', f'{performance["macro_f1"]*100:.2f}%'],
        ['í•™ìŠµ ì‹œê°„', '~30ë¶„ (ì¶”ì •)', f'{training_time/60:.1f}ë¶„']
    ]
    
    for row in comparison:
        print(f"{row[0]:20} | {row[1]:20} | {row[2]:20}")
    
    print("="*70)
    
    print(f"\nğŸ¯ ê²°ë¡ :")
    if performance['accuracy'] >= 0.6309:
        print(f"  âœ… Refer ëª¨ë¸ ì„±ëŠ¥ ë‹¬ì„± ë˜ëŠ” ì´ˆê³¼!")
    else:
        gap = (0.6309 - performance['accuracy']) * 100
        print(f"  âš ï¸ Refer ëª¨ë¸ë³´ë‹¤ {gap:.2f}%p ë‚®ìŒ")
        print(f"  ì›ì¸: ë°ì´í„° í¬ê¸° 32ë°° (ë…¸ì´ì¦ˆ ì¦ê°€)")
    
    print(f"\n  ğŸ’¡ ê°œì„  ë°©ì•ˆ:")
    print(f"     1. ExtraTrees ì§ì ‘ ì‚¬ìš© (CPU)")
    print(f"     2. ë°ì´í„° í’ˆì§ˆ í–¥ìƒ (ì´ìƒì¹˜ ì œê±°)")
    print(f"     3. ì•™ìƒë¸” (ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©)")


if __name__ == '__main__':
    main()
