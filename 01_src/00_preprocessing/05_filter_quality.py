"""
ë°©ë²•4: í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§ (ê°•ë ¥)
- ê±°ë˜ íšŸìˆ˜, ì¹´í…Œê³ ë¦¬ ë‹¤ì–‘ì„±, ê¸ˆì•¡ ì¼ê´€ì„± ì¢…í•©
- ìƒìœ„ 30%ë§Œ ì„ íƒ
- ëª©í‘œ: 60%+ Accuracy
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score
from sklearn.utils.class_weight import compute_sample_weight
import joblib
import json
import os
from datetime import datetime


def calculate_quality_score(df):
    """ì‚¬ìš©ìë³„ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
    print("\ní’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì¤‘...")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ í™•ì¸
    has_amount = 'Amount_scaled' in df.columns
    has_category = 'Current_Category' in df.columns
    has_sequence = 'Transaction_Sequence_scaled' in df.columns
    
    user_metrics = []
    
    for user in df['User'].unique():
        user_data = df[df['User'] == user]
        
        # 1. ê±°ë˜ íšŸìˆ˜ (40%)
        tx_count = len(user_data)
        tx_score = min(tx_count / 10000, 1.0)
        
        # 2. ì¹´í…Œê³ ë¦¬ ë‹¤ì–‘ì„± (40%)
        if has_category:
            unique_cats = user_data['Current_Category'].nunique()
            diversity_score = unique_cats / 6.0
        else:
            # Next_Categoryë¡œ ëŒ€ì²´
            unique_cats = user_data['Next_Category'].nunique()
            diversity_score = unique_cats / 6.0
        
        # 3. ê¸ˆì•¡ ì¼ê´€ì„± (10%) - scaled ì‚¬ìš©
        if has_amount:
            amounts = user_data['Amount_scaled'].values
            if len(amounts) > 1:
                cv = np.std(amounts) / (abs(np.mean(amounts)) + 1e-10)
                consistency_score = 1.0 / (1.0 + abs(cv))
            else:
                consistency_score = 0.5
        else:
            consistency_score = 0.5
        
        # 4. í™œë™ ì§€ì†ì„± (10%)
        persistence_score = 0.5  # ë‹¨ìˆœí™”
        
        # ì¢…í•© ì ìˆ˜ (ê±°ë˜ íšŸìˆ˜ì™€ ë‹¤ì–‘ì„± ì¤‘ì‹¬)
        final_score = (
            tx_score * 0.4 +
            diversity_score * 0.4 +
            consistency_score * 0.1 +
            persistence_score * 0.1
        )
        
        user_metrics.append({
            'User': user,
            'tx_count': tx_count,
            'diversity': unique_cats,
            'consistency': consistency_score,
            'persistence': persistence_score,
            'quality_score': final_score
        })
    
    metrics_df = pd.DataFrame(user_metrics)
    
    print(f"\ní’ˆì§ˆ ì ìˆ˜ í†µê³„:")
    print(f"  í‰ê· : {metrics_df['quality_score'].mean():.3f}")
    print(f"  ì¤‘ì•™ê°’: {metrics_df['quality_score'].median():.3f}")
    print(f"  ìµœëŒ€: {metrics_df['quality_score'].max():.3f}")
    print(f"  ìµœì†Œ: {metrics_df['quality_score'].min():.3f}")
    
    return metrics_df


def filter_by_quality(df, top_percent=30):
    """í’ˆì§ˆ ì ìˆ˜ ìƒìœ„ N% ì„ íƒ"""
    print("="*70)
    print(f"í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§ (ìƒìœ„ {top_percent}%)")
    print("="*70)
    
    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    quality_df = calculate_quality_score(df)
    
    # ìƒìœ„ N% ì„ íƒ
    threshold = quality_df['quality_score'].quantile(1 - top_percent/100)
    high_quality_users = quality_df[quality_df['quality_score'] >= threshold]['User'].values
    
    print(f"\ní•„í„°ë§ ê²°ê³¼:")
    print(f"  ì „ì²´ ì‚¬ìš©ì: {len(quality_df)}ëª…")
    print(f"  ì„ íƒëœ ì‚¬ìš©ì: {len(high_quality_users)}ëª…")
    print(f"  í’ˆì§ˆ ì ìˆ˜ ì„ê³„ê°’: {threshold:.3f}")
    
    # í•„í„°ë§
    filtered_df = df[df['User'].isin(high_quality_users)].copy()
    
    print(f"\në°ì´í„° í¬ê¸°:")
    print(f"  ì›ë³¸: {len(df):,}ê±´")
    print(f"  í•„í„°ë§ í›„: {len(filtered_df):,}ê±´")
    print(f"  ê°ì†Œìœ¨: {(1 - len(filtered_df)/len(df))*100:.1f}%")
    
    # ì„ íƒëœ ì‚¬ìš©ì í†µê³„
    selected_quality = quality_df[quality_df['User'].isin(high_quality_users)]
    print(f"\nì„ íƒëœ ì‚¬ìš©ì í’ˆì§ˆ:")
    print(f"  í‰ê·  ê±°ë˜ ìˆ˜: {selected_quality['tx_count'].mean():.0f}ê±´")
    print(f"  í‰ê·  ì¹´í…Œê³ ë¦¬ ìˆ˜: {selected_quality['diversity'].mean():.1f}ê°œ")
    print(f"  í‰ê·  í’ˆì§ˆ ì ìˆ˜: {selected_quality['quality_score'].mean():.3f}")
    
    return filtered_df


def train_with_quality_filtered(df):
    """í’ˆì§ˆ í•„í„°ë§ëœ ë°ì´í„°ë¡œ í•™ìŠµ"""
    print("\n" + "="*70)
    print("í’ˆì§ˆ í•„í„°ë§ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ")
    print("="*70)
    
    # í”¼ì²˜ ë¡œë“œ
    feature_file = '02_data/01_processed/selected_features_enhanced.json'
    with open(feature_file, 'r', encoding='utf-8') as f:
        feature_info = json.load(f)
    
    selected_features = [f"{f}_scaled" for f in feature_info['selected_features']]
    
    # ë°ì´í„° ì¤€ë¹„
    X = df[selected_features].values.astype('float32')
    y = df['Next_Category_encoded'].values.astype('int32')
    
    print(f"\ní”¼ì²˜: {len(selected_features)}ê°œ")
    print(f"ìƒ˜í”Œ: {len(X):,}ê°œ")
    
    # ë¶ˆê· í˜• ë³´ì •
    sample_weights = compute_sample_weight('balanced', y)
    
    # ë¶„í• 
    X_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(
        X, y, sample_weights, test_size=0.2, random_state=42, stratify=y
    )
    
    # í•™ìŠµ
    model = xgb.XGBClassifier(
        device='cuda',
        tree_method='hist',
        max_depth=10,
        learning_rate=0.1,
        n_estimators=300,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42
    )
    
    print("\ní•™ìŠµ ì‹œì‘...")
    start_time = datetime.now()
    model.fit(X_train, y_train, sample_weight=sw_train, verbose=False)
    training_time = (datetime.now() - start_time).total_seconds()
    print(f"í•™ìŠµ ì™„ë£Œ: {training_time:.1f}ì´ˆ")
    
    # í‰ê°€
    y_pred = model.predict(X_test)
    
    acc = accuracy_score(y_test, y_pred)
    f1_macro = f1_score(y_test, y_pred, average='macro')
    f1_weighted = f1_score(y_test, y_pred, average='weighted')
    
    print(f"\nì„±ëŠ¥:")
    print(f"  Accuracy:      {acc:.4f} ({acc*100:.2f}%)")
    print(f"  Macro F1:      {f1_macro:.4f}")
    print(f"  Weighted F1:   {f1_weighted:.4f}")
    
    print(f"\në¹„êµ:")
    print(f"  ê¸°ì¡´:       45.90% Acc")
    print(f"  í’ˆì§ˆ í•„í„°:  {acc*100:.2f}% Acc")
    print(f"  ê°œì„ :       {(acc-0.4590)*100:+.2f}%p")
    
    print(f"\nRefer ëŒ€ë¹„:")
    print(f"  Refer:      63.09% Acc")
    print(f"  í’ˆì§ˆ í•„í„°:  {acc*100:.2f}% Acc")
    print(f"  ê°­:         {(acc-0.6309)*100:+.2f}%p")
    
    # ì¹´í…Œê³ ë¦¬ë³„
    categories = ['êµí†µ', 'ìƒí™œ', 'ì‡¼í•‘', 'ì‹ë£Œí’ˆ', 'ì™¸ì‹', 'ì£¼ìœ ']
    print(f"\nì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥:")
    print(classification_report(y_test, y_pred, target_names=categories, digits=4))
    
    # ì €ì¥
    output_dir = '03_models/12_quality_filtered'
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    model_file = os.path.join(output_dir, f'quality_filtered_top30_{timestamp}.joblib')
    joblib.dump(model, model_file)
    
    print(f"\nâœ… ëª¨ë¸ ì €ì¥: {model_file}")
    
    return acc, f1_macro


def main():
    print("="*70)
    print("ë°©ë²•4: í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§")
    print("="*70)
    
    # ì›ë³¸ ë¡œë“œ
    df = pd.read_csv('02_data/01_processed/preprocessed_enhanced.csv')
    print(f"\nì›ë³¸: {len(df):,}ê±´")
    
    # í’ˆì§ˆ í•„í„°ë§ (ìƒìœ„ 30%)
    filtered_df = filter_by_quality(df, top_percent=30)
    
    # í•™ìŠµ
    acc, f1 = train_with_quality_filtered(filtered_df)
    
    # ê²°ë¡ 
    print("\n" + "="*70)
    print("ìµœì¢… ê²°ë¡ ")
    print("="*70)
    
    if acc >= 0.60:
        print(f"\nğŸ‰ ì„±ê³µ! {acc*100:.2f}% Accuracy")
        print(f"   Refer ìˆ˜ì¤€ ë‹¬ì„±!")
    elif acc >= 0.55:
        print(f"\nâœ… ê°œì„ ! {acc*100:.2f}% Accuracy")
        print(f"   ê¸°ì¡´ ëŒ€ë¹„ {(acc-0.4590)*100:+.2f}%p í–¥ìƒ")
    else:
        print(f"\nâš ï¸  {acc*100:.2f}% Accuracy")
print(f"   ì¶”ê°€ ê°œì„  í•„ìš”")


if __name__ == '__main__':
    main()
