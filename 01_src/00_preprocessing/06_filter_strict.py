"""
ì›ë³¸ ì—†ì´ í™œë™ì„± í•„í„°ë§
Transaction_Sequence ë²”ìœ„ë¥¼ í™œìš©í•œ ì›”ë³„ í™œë™ ì¶”ì •
ì¡°ê±´: ì‚¬ìš©ìë‹¹ ê±°ë˜ ìˆ˜ ê¸°ë°˜ (ì›” 10ê±´ Ã— 5ê°œì›” = ìµœì†Œ 50ê±´ ì´ìƒ, ì‹¤ì œë¡œëŠ” ë” ì—„ê²©í•˜ê²Œ)
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score
from sklearn.utils.class_weight import compute_sample_weight
import joblib
import json
import os
from datetime import datetime


def filter_active_users_strict(df, min_transactions=200):
    """
    ì—„ê²©í•œ í™œë™ì„± í•„í„°ë§
    - ì‚¬ìš©ìë‹¹ ìµœì†Œ ê±°ë˜ ìˆ˜
    - ì¹´í…Œê³ ë¦¬ ë‹¤ì–‘ì„± (ìµœì†Œ 4ê°œ ì¹´í…Œê³ ë¦¬)
    """
    print("="*70)
    print("ì—„ê²©í•œ í™œë™ì„± í•„í„°ë§")
    print("="*70)
    
    print(f"\nì¡°ê±´:")
    print(f"  1. ì‚¬ìš©ìë‹¹ ìµœì†Œ {min_transactions}ê±´ ê±°ë˜")
    print(f"  2. ìµœì†Œ 4ê°œ ì´ìƒ ì¹´í…Œê³ ë¦¬ ì‚¬ìš©")
    
    # ì‚¬ìš©ìë³„ í†µê³„
    user_stats = df.groupby('User').agg({
        'Next_Category': ['count', 'nunique']
    })
    user_stats.columns = ['tx_count', 'cat_count']
    
    # ì¡°ê±´ ì ìš©
    active_users = user_stats[
        (user_stats['tx_count'] >= min_transactions) &
        (user_stats['cat_count'] >= 4)
    ].index
    
    print(f"\ní•„í„°ë§ ê²°ê³¼:")
    print(f"  ì „ì²´ ì‚¬ìš©ì: {len(user_stats)}ëª…")
    print(f"  í™œë™ì  ì‚¬ìš©ì: {len(active_users)}ëª… ({len(active_users)/len(user_stats)*100:.1f}%)")
    
    # ì„ íƒëœ ì‚¬ìš©ì í†µê³„
    selected_stats = user_stats.loc[active_users]
    print(f"\nì„ íƒëœ ì‚¬ìš©ì íŠ¹ì§•:")
    print(f"  í‰ê·  ê±°ë˜ ìˆ˜: {selected_stats['tx_count'].mean():.0f}ê±´")
    print(f"  ì¤‘ì•™ê°’ ê±°ë˜ ìˆ˜: {selected_stats['tx_count'].median():.0f}ê±´")
    print(f"  í‰ê·  ì¹´í…Œê³ ë¦¬ ìˆ˜: {selected_stats['cat_count'].mean():.1f}ê°œ")
    
    # í•„í„°ë§
    filtered_df = df[df['User'].isin(active_users)].copy()
    
    print(f"\në°ì´í„° ë³€í™”:")
    print(f"  ì›ë³¸: {len(df):,}ê±´")
    print(f"  í•„í„°ë§: {len(filtered_df):,}ê±´")
    print(f"  ê°ì†Œìœ¨: {(1 - len(filtered_df)/len(df))*100:.1f}%")
    
    return filtered_df


def train_with_filtered(df):
    """í•„í„°ë§ëœ ë°ì´í„°ë¡œ í•™ìŠµ"""
    print("\n" + "="*70)
    print("í•„í„°ë§ëœ ë°ì´í„°ë¡œ XGBoost í•™ìŠµ")
    print("="*70)
    
    # í”¼ì²˜ ë¡œë“œ
    feature_file = '02_data/01_processed/selected_features_enhanced.json'
    with open(feature_file, 'r') as f:
        feature_info = json.load(f)
    
    selected_features = [f"{f}_scaled" for f in feature_info['selected_features']]
    
    X = df[selected_features].values.astype('float32')
    y = df['Next_Category_encoded'].values.astype('int32')
    
    print(f"\në°ì´í„°:")
    print(f"  ìƒ˜í”Œ: {len(X):,}ê°œ")
    print(f"  í”¼ì²˜: {len(selected_features)}ê°œ")
    print(f"  ë©”ëª¨ë¦¬: {X.nbytes / 1024**2:.1f} MB")
    
    # ë¶ˆê· í˜• ë³´ì •
    sample_weights = compute_sample_weight('balanced', y)
    
    # ë¶„í• 
    X_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(
        X, y, sample_weights, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\në¶„í• :")
    print(f"  í•™ìŠµ: {len(X_train):,}ê°œ")
    print(f"  í…ŒìŠ¤íŠ¸: {len(X_test):,}ê°œ")
    
    # í•™ìŠµ
    print("\nXGBoost í•™ìŠµ (GPU)...")
    model = xgb.XGBClassifier(
        device='cuda',
        tree_method='hist',
        max_depth=10,
        learning_rate=0.1,
        n_estimators=300,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42
    )
    
    start_time = datetime.now()
    model.fit(X_train, y_train, sample_weight=sw_train, verbose=False)
    training_time = (datetime.now() - start_time).total_seconds()
    
    print(f"í•™ìŠµ ì™„ë£Œ: {training_time:.1f}ì´ˆ")
    
    # í‰ê°€
    print("\n" + "="*70)
    print("ì„±ëŠ¥ í‰ê°€")
    print("="*70)
    
    y_pred = model.predict(X_test)
    
    acc = accuracy_score(y_test, y_pred)
    f1_macro = f1_score(y_test, y_pred, average='macro')
    f1_weighted = f1_score(y_test, y_pred, average='weighted')
    
    print(f"\nì„±ëŠ¥:")
    print(f"  Accuracy:      {acc:.4f} ({acc*100:.2f}%)")
    print(f"  Macro F1:      {f1_macro:.4f}")
    print(f"  Weighted F1:   {f1_weighted:.4f}")
    
    # ë¹„êµ
    print(f"\në¹„êµ:")
    print(f"  ê¸°ì¡´ (ì „ì²´):      45.90% Acc, 44.93% F1")
    print(f"  í’ˆì§ˆ í•„í„° (30%):  48.00% Acc, 45.96% F1")
    print(f"  í™œë™ì„± í•„í„°:      {acc*100:.2f}% Acc, {f1_macro*100:.2f}% F1")
    
    print(f"\nRefer ëŒ€ë¹„:")
    print(f"  Refer:         63.09% Acc, 54.86% F1")
    print(f"  í™œë™ì„± í•„í„°:   {acc*100:.2f}% Acc, {f1_macro*100:.2f}% F1")
    print(f"  ê°­:            {(acc-0.6309)*100:+.2f}%p Acc, {(f1_macro-0.5486)*100:+.2f}%p F1")
    
    # ì¹´í…Œê³ ë¦¬ë³„
    categories = ['êµí†µ', 'ìƒí™œ', 'ì‡¼í•‘', 'ì‹ë£Œí’ˆ', 'ì™¸ì‹', 'ì£¼ìœ ']
    print(f"\nì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥:")
    report = classification_report(y_test, y_pred, target_names=categories, digits=4, output_dict=True)
    print(classification_report(y_test, y_pred, target_names=categories, digits=4))
    
    # ìƒí™œ ì¹´í…Œê³ ë¦¬ ê°•ì¡°
    life_f1 = report['ìƒí™œ']['f1-score']
    print(f"\nğŸ” ìƒí™œ ì¹´í…Œê³ ë¦¬ F1: {life_f1:.4f}")
    print(f"   ì´ì „ ëŒ€ë¹„: {(life_f1 - 0.2654)*100:+.2f}%p")
    
    # ì €ì¥
    output_dir = '03_models/13_strict_active'
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    model_file = os.path.join(output_dir, f'strict_active_{timestamp}.joblib')
    joblib.dump(model, model_file)
    
    metadata = {
        'model_name': 'xgboost_strict_active',
        'filtering': 'min_200_tx_and_4_categories',
        'original_samples': 6443429,
        'filtered_samples': len(df),
        'reduction_rate': f"{(1 - len(df)/6443429)*100:.1f}%",
        'performance': {
            'accuracy': float(acc),
            'macro_f1': float(f1_macro),
            'weighted_f1': float(f1_weighted),
            'life_category_f1': float(life_f1)
        },
        'comparison': {
            'vs_baseline_acc': float(acc - 0.4590),
            'vs_quality_filter_acc': float(acc - 0.4800),
            'vs_refer_acc': float(acc - 0.6309)
        },
        'training_time': training_time,
        'created_at': datetime.now().isoformat()
    }
    
    metadata_file = os.path.join(output_dir, f'metadata_{timestamp}.json')
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ëª¨ë¸ ì €ì¥: {model_file}")
    print(f"âœ… ë©”íƒ€ë°ì´í„°: {metadata_file}")
    
    return acc, f1_macro


def main():
    print("="*70)
    print("ì—„ê²©í•œ í™œë™ì„± í•„í„°ë§ + í•™ìŠµ")
    print("="*70)
    print("\nëª©í‘œ: ê³ í’ˆì§ˆ í™œë™ ì‚¬ìš©ìë§Œ ì„ ë³„")
    
    # ë°ì´í„° ë¡œë“œ
    print("\në°ì´í„° ë¡œë“œ...")
    df = pd.read_csv('02_data/01_processed/preprocessed_enhanced.csv')
    print(f"ì›ë³¸: {len(df):,}ê±´")
    
    # í•„í„°ë§ (ìµœì†Œ 200ê±´)
    filtered_df = filter_active_users_strict(df, min_transactions=200)
    
    # í•™ìŠµ
    acc, f1 = train_with_filtered(filtered_df)
    
    # ê²°ë¡ 
    print("\n" + "="*70)
    print("ìµœì¢… ê²°ë¡ ")
    print("="*70)
    
    if acc >= 0.60:
        print(f"\nğŸ‰ ëŒ€ì„±ê³µ! {acc*100:.2f}% Accuracy")
        print(f"   Refer ìˆ˜ì¤€ ë‹¬ì„±!")
    elif acc >= 0.55:
        print(f"\nâœ… ì„±ê³µ! {acc*100:.2f}% Accuracy")
        print(f"   Referê¹Œì§€ {(0.6309-acc)*100:.2f}%p")
    elif acc >= 0.50:
        print(f"\nâœ¨ ê°œì„ ! {acc*100:.2f}% Accuracy")
        print(f"   ê¸°ì¡´ ëŒ€ë¹„ {(acc-0.4590)*100:+.2f}%p")
    else:
        print(f"\nâš ï¸  {acc*100:.2f}% Accuracy")
        print(f"   ì¶”ê°€ ê°œì„  í•„ìš”")


if __name__ == '__main__':
    main()
