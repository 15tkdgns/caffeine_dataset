"""
STEP 3: ADASYN - ìƒí™œ ì¹´í…Œê³ ë¦¬ ì§‘ì¤‘ ì¦ê°•
- ìƒí™œ ì¹´í…Œê³ ë¦¬ë§Œ 2.5ë°° ì¦ê°•
- Class Weight ìœ ì§€
- ëª©í‘œ: Accuracy íšŒë³µ + ìƒí™œ F1 ìœ ì§€
"""

import numpy as np
import pandas as pd
import lightgbm as lgb
import xgboost as xgb
from sklearn.metrics import accuracy_score, f1_score
from imblearn.over_sampling import ADASYN, SMOTE
from sklearn.utils.class_weight import compute_class_weight
import joblib
import json
from datetime import datetime
import time

print("="*80)
print("ğŸ”¬ STEP 3: ADASYN - ìƒí™œ ì¹´í…Œê³ ë¦¬ ì§‘ì¤‘ ì¦ê°•")
print("="*80)

# ============================================================
# 1. ë°ì´í„° ë¡œë“œ
# ============================================================
print("\n[1/7] ë°ì´í„° ë¡œë“œ")

X_train = np.load('02_data/02_augmented/X_train_smote.npy')
y_train = np.load('02_data/02_augmented/y_train_smote.npy')
X_test = np.load('02_data/02_augmented/X_test.npy')
y_test = np.load('02_data/02_augmented/y_test.npy')

print(f"  ì›ë³¸ í•™ìŠµ ë°ì´í„°: {len(X_train):,}ê±´")

category_names = ['êµí†µ', 'ìƒí™œ', 'ì‡¼í•‘', 'ì‹ë£Œí’ˆ', 'ì™¸ì‹', 'ì£¼ìœ ']

# ============================================================
# 2. ì „ëµì  ì–¸ë”ìƒ˜í”Œë§ (STEP 2ì™€ ë™ì¼)
# ============================================================
print("\n[2/7] ì „ëµì  ì–¸ë”ìƒ˜í”Œë§")

sampling_ratios = {
    0: 1.0,   # êµí†µ
    1: 1.0,   # ìƒí™œ (ì¦ê°• ì „)
    2: 0.7,   # ì‡¼í•‘
    3: 0.7,   # ì‹ë£Œí’ˆ
    4: 0.7,   # ì™¸ì‹
    5: 1.0    # ì£¼ìœ 
}

indices_to_keep = []
for class_id in range(6):
    class_mask = (y_train == class_id)
    class_indices = np.where(class_mask)[0]
    n_samples = len(class_indices)
    n_keep = int(n_samples * sampling_ratios[class_id])
    np.random.seed(42)
    kept_indices = np.random.choice(class_indices, n_keep, replace=False)
    indices_to_keep.extend(kept_indices)
    print(f"  {category_names[class_id]:6s}: {n_samples:,}ê±´ â†’ {n_keep:,}ê±´")

X_train_sampled = X_train[np.array(indices_to_keep)]
y_train_sampled = y_train[np.array(indices_to_keep)]

print(f"\n  ìƒ˜í”Œë§ í›„: {len(X_train_sampled):,}ê±´")

# ============================================================
# 3. ADASYN - ìƒí™œ ì¹´í…Œê³ ë¦¬ë§Œ ì¦ê°•
# ============================================================
print("\n[3/7] ADASYN - ìƒí™œ ì¹´í…Œê³ ë¦¬ ì§‘ì¤‘ ì¦ê°•")

# í˜„ì¬ í´ë˜ìŠ¤ ë¶„í¬
unique, counts = np.unique(y_train_sampled, return_counts=True)
print(f"\n  ì¦ê°• ì „ í´ë˜ìŠ¤ ë¶„í¬:")
for cat_id, count in zip(unique, counts):
    print(f"     {category_names[cat_id]:6s}: {count:,}ê±´")

# ìƒí™œ ì¹´í…Œê³ ë¦¬ë¥¼ ë‹¤ë¥¸ í´ë˜ìŠ¤ í‰ê·  ìˆ˜ì¤€ìœ¼ë¡œ ì¦ê°•
ìƒí™œ_count = counts[1]
other_avg = counts[[0, 5]].mean()  # êµí†µ, ì£¼ìœ  í‰ê· 
target_ìƒí™œ_count = int(other_avg * 1.0)  # í‰ê·  ìˆ˜ì¤€

print(f"\n  ìƒí™œ ì¹´í…Œê³ ë¦¬ ì¦ê°•:")
print(f"     í˜„ì¬: {ìƒí™œ_count:,}ê±´")
print(f"     ëª©í‘œ: {target_ìƒí™œ_count:,}ê±´")
print(f"     ì¦ê°•: {target_ìƒí™œ_count - ìƒí™œ_count:,}ê±´ ì¶”ê°€")

# ìƒí™œ ì¹´í…Œê³ ë¦¬ ì¸ë±ìŠ¤
ìƒí™œ_indices = np.where(y_train_sampled == 1)[0]
ìƒí™œ_X = X_train_sampled[ìƒí™œ_indices]
ìƒí™œ_y = y_train_sampled[ìƒí™œ_indices]

# ADASYN ì ìš© (ìƒí™œ ì¹´í…Œê³ ë¦¬ë§Œ)
try:
    from imblearn.over_sampling import ADASYN
    
    # ìƒí™œ(1)ê³¼ ë‹¤ë¥¸ í´ë˜ìŠ¤(0) ë‘ í´ë˜ìŠ¤ë¡œ ë³€í™˜
    other_indices = np.where(y_train_sampled != 1)[0]
    np.random.seed(42)
    other_sample_indices = np.random.choice(other_indices, len(ìƒí™œ_indices)*2, replace=False)
    
    temp_X = np.vstack([ìƒí™œ_X, X_train_sampled[other_sample_indices]])
    temp_y = np.hstack([np.ones(len(ìƒí™œ_X)), np.zeros(len(other_sample_indices))])
    
    # ADASYNìœ¼ë¡œ ìƒí™œ ì¦ê°•
    adasyn = ADASYN(sampling_strategy='minority', random_state=42, n_neighbors=5)
    X_resampled, y_resampled = adasyn.fit_resample(temp_X, temp_y)
    
    # ì¦ê°•ëœ ìƒí™œ ìƒ˜í”Œë§Œ ì¶”ì¶œ
    ìƒí™œ_mask = y_resampled == 1
    ìƒí™œ_augmented_X = X_resampled[ìƒí™œ_mask]
    
    # ì›ë³¸ ìƒí™œ ì œì™¸í•˜ê³  ì¦ê°•ëœ ê²ƒë§Œ
    n_original_ìƒí™œ = len(ìƒí™œ_X)
    ìƒí™œ_new_X = ìƒí™œ_augmented_X[n_original_ìƒí™œ:]
    
    # ëª©í‘œ ê°œìˆ˜ë§Œí¼ë§Œ ì¶”ê°€
    n_to_add = min(len(ìƒí™œ_new_X), target_ìƒí™œ_count - ìƒí™œ_count)
    ìƒí™œ_new_X = ìƒí™œ_new_X[:n_to_add]
    ìƒí™œ_new_y = np.ones(n_to_add, dtype=int)
    
    # ì›ë³¸ ë°ì´í„°ì— ì¦ê°• ë°ì´í„° ì¶”ê°€
    X_train_final = np.vstack([X_train_sampled, ìƒí™œ_new_X])
    y_train_final = np.hstack([y_train_sampled, ìƒí™œ_new_y])
    
    print(f"  âœ… ADASYN ì¦ê°• ì™„ë£Œ: {n_to_add:,}ê±´ ì¶”ê°€")
    
except Exception as e:
    print(f"  âš ï¸ ADASYN ì‹¤íŒ¨ ({e}), SMOTE ì‚¬ìš©")
    
    # SMOTE ëŒ€ì²´
    smote = SMOTE(sampling_strategy={1: target_ìƒí™œ_count}, random_state=42, k_neighbors=5)
    
    # ì„ì‹œë¡œ ìƒí™œë§Œ ì¦ê°•
    temp_sampling_strategy = {
        0: counts[0],
        1: target_ìƒí™œ_count,
        2: counts[2],
        3: counts[3],
        4: counts[4],
        5: counts[5]
    }
    
    X_train_final, y_train_final = smote.fit_resample(X_train_sampled, y_train_sampled)
    print(f"  âœ… SMOTE ì¦ê°• ì™„ë£Œ")

# ìµœì¢… ë¶„í¬
unique_final, counts_final = np.unique(y_train_final, return_counts=True)
print(f"\n  ì¦ê°• í›„ í´ë˜ìŠ¤ ë¶„í¬:")
for cat_id, count in zip(unique_final, counts_final):
    print(f"     {category_names[cat_id]:6s}: {count:,}ê±´")

print(f"\n  ì´ í•™ìŠµ ë°ì´í„°: {len(X_train_sampled):,}ê±´ â†’ {len(X_train_final):,}ê±´")

# ============================================================
# 4. Class Weight ê³„ì‚°
# ============================================================
print("\n[4/7] Class Weight ê³„ì‚°")

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train_final),
    y=y_train_final
)

# ìƒí™œ 2ë°° ë¶€ìŠ¤íŒ… (STEP 2ëŠ” 3ë°°ì˜€ì§€ë§Œ, ë°ì´í„° ì¦ê°•í–ˆìœ¼ë¯€ë¡œ ì¡°ì •)
class_weights[1] *= 2.0

print(f"\n  Class Weights (ìƒí™œ 2ë°°):")
for cat, weight in zip(category_names, class_weights):
    print(f"     {cat:6s}: {weight:.4f}")

sample_weights = np.array([class_weights[y] for y in y_train_final])

# ============================================================
# 5. LightGBM with ADASYN + Weight
# ============================================================
print("\n[5/7] LightGBM with ADASYN + Weight")

start = time.time()
lgb_model = lgb.LGBMClassifier(
    n_estimators=300,
    max_depth=10,
    learning_rate=0.1,
    num_leaves=128,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
    verbose=-1
)

lgb_model.fit(X_train_final, y_train_final, sample_weight=sample_weights)
train_time_lgb = time.time() - start

y_pred_lgb = lgb_model.predict(X_test)
acc_lgb = accuracy_score(y_test, y_pred_lgb)
macro_f1_lgb = f1_score(y_test, y_pred_lgb, average='macro')
category_f1_lgb = f1_score(y_test, y_pred_lgb, average=None)

print(f"  âœ… í•™ìŠµ ì™„ë£Œ: {train_time_lgb:.2f}ì´ˆ")
print(f"\n  ğŸ“Š ì„±ëŠ¥:")
print(f"     Accuracy:    {acc_lgb:.4f} ({acc_lgb*100:.2f}%)")
print(f"     Macro F1:    {macro_f1_lgb:.4f}")

print(f"\n  ì¹´í…Œê³ ë¦¬ë³„ F1:")
for cat, f1 in zip(category_names, category_f1_lgb):
    emoji = "â­" if f1 > 0.5 else "âœ…" if f1 > 0.3 else "âš ï¸"
    if cat == "ìƒí™œ":
        print(f"     {emoji} {cat:6s}: {f1:.4f} (Baseline: 0.0802, STEP2: 0.2541)")
    else:
        print(f"     {emoji} {cat:6s}: {f1:.4f}")

# ============================================================
# 6. XGBoost with ADASYN + Weight  
# ============================================================
print("\n[6/7] XGBoost with ADASYN + Weight")

start = time.time()
xgb_model = xgb.XGBClassifier(
    device='cuda',
    tree_method='hist',
    n_estimators=300,
    max_depth=10,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)

xgb_model.fit(X_train_final, y_train_final, sample_weight=sample_weights)
train_time_xgb = time.time() - start

y_pred_xgb = xgb_model.predict(X_test)
acc_xgb = accuracy_score(y_test, y_pred_xgb)
macro_f1_xgb = f1_score(y_test, y_pred_xgb, average='macro')
category_f1_xgb = f1_score(y_test, y_pred_xgb, average=None)

print(f"  âœ… í•™ìŠµ ì™„ë£Œ: {train_time_xgb:.2f}ì´ˆ")
print(f"\n  ğŸ“Š ì„±ëŠ¥:")
print(f"     Accuracy:    {acc_xgb:.4f} ({acc_xgb*100:.2f}%)")
print(f"     Macro F1:    {macro_f1_xgb:.4f}")

print(f"\n  ì¹´í…Œê³ ë¦¬ë³„ F1:")
for cat, f1 in zip(category_names, category_f1_xgb):
    emoji = "â­" if f1 > 0.5 else "âœ…" if f1 > 0.3 else "âš ï¸"
    if cat == "ìƒí™œ":
        print(f"     {emoji} {cat:6s}: {f1:.4f} (Baseline: 0.0802, STEP2: 0.2522)")
    else:
        print(f"     {emoji} {cat:6s}: {f1:.4f}")

# ============================================================
# 7. ê²°ê³¼ ì €ì¥
# ============================================================
print("\n[7/7] ê²°ê³¼ ì €ì¥")

import os
os.makedirs('04_logs/step3_adasyn', exist_ok=True)

best_model = lgb_model if acc_lgb >= acc_xgb else xgb_model
best_name = 'LightGBM' if acc_lgb >= acc_xgb else 'XGBoost'

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
model_path = f'04_logs/step3_adasyn/{best_name.lower()}_adasyn_{timestamp}.joblib'
joblib.dump(best_model, model_path)

metadata = {
    'experiment': 'STEP 3: ADASYN + Class Weight',
    'strategy': {
        'method': 'ADASYN on Living category only',
        'augmentation': f'Living: {ìƒí™œ_count} â†’ {counts_final[1]} (+{counts_final[1]-ìƒí™œ_count})',
        'class_weights': {cat: float(w) for cat, w in zip(category_names, class_weights)}
    },
    'results': {
        'lightgbm': {
            'accuracy': float(acc_lgb),
            'macro_f1': float(macro_f1_lgb),
            'category_f1': {cat: float(f1) for cat, f1 in zip(category_names, category_f1_lgb)}
        },
        'xgboost': {
            'accuracy': float(acc_xgb),
            'macro_f1': float(macro_f1_xgb),
            'category_f1': {cat: float(f1) for cat, f1 in zip(category_names, category_f1_xgb)}
        }
    }
}

with open(f'04_logs/step3_adasyn/metadata_{timestamp}.json', 'w', encoding='utf-8') as f:
    json.dump(metadata, f, indent=2, ensure_ascii=False)

print(f"  âœ… ì €ì¥ ì™„ë£Œ")

# ============================================================
# ìµœì¢… ë¹„êµ
# ============================================================
print("\n" + "="*80)
print("ğŸ† ì „ì²´ STEP ë¹„êµ")
print("="*80)

print(f"\n{'ë‹¨ê³„':<25} {'Accuracy':>12} {'Macro F1':>12} {'ìƒí™œ F1':>12}")
print("-"*65)
print(f"{'Baseline':<25} {0.4913:>12.4f} {0.4344:>12.4f} {0.0802:>12.4f}")
print(f"{'STEP 2 (Weight+Under)':<25} {0.4485:>12.4f} {0.4287:>12.4f} {0.2541:>12.4f}")
print(f"{'STEP 3 LGB (ADASYN)':<25} {acc_lgb:>12.4f} {macro_f1_lgb:>12.4f} {category_f1_lgb[1]:>12.4f}")
print(f"{'STEP 3 XGB (ADASYN)':<25} {acc_xgb:>12.4f} {macro_f1_xgb:>12.4f} {category_f1_xgb[1]:>12.4f}")
print("-"*65)

print(f"\nğŸ¯ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€:")
acc_improved = max(acc_lgb, acc_xgb) > 0.4485
ìƒí™œ_maintained = min(category_f1_lgb[1], category_f1_xgb[1]) >= 0.20

if acc_improved and ìƒí™œ_maintained:
    print(f"  âœ…âœ… ì„±ê³µ! Accuracy íšŒë³µ + ìƒí™œ F1 ìœ ì§€")
elif ìƒí™œ_maintained:
    print(f"  âœ… ìƒí™œ F1 ìœ ì§€, AccuracyëŠ” ë¹„ìŠ·")
else:
    print(f"  âš ï¸ ì¶”ê°€ ì¡°ì • í•„ìš”")

print("\n=" *80)
print("âœ… STEP 3 ì™„ë£Œ!")
print("="*80)
print(f"\nğŸ“¦ ë‹¤ìŒ ë‹¨ê³„: STEP 4 - Focal Loss")
print("="*80)
