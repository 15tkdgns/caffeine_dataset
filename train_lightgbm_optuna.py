"""
LightGBM Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹ (GPU)
í”„ë¡œë•ì…˜ ëª¨ë¸ ì„±ëŠ¥ ê°œì„  ëª©í‘œ
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import optuna
import joblib
import json
from datetime import datetime
from sklearn.metrics import accuracy_score, f1_score
import time

print("="*80)
print("ğŸ”¬ LightGBM Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹ (GPU)")
print("="*80)

# ============================================================
# 1. ë°ì´í„° ë¡œë“œ
# ============================================================
print("\n[1/5] ë°ì´í„° ë¡œë“œ")

X_train = np.load('02_data/02_augmented/X_train_smote.npy')
y_train = np.load('02_data/02_augmented/y_train_smote.npy')
X_test = np.load('02_data/02_augmented/X_test.npy')
y_test = np.load('02_data/02_augmented/y_test.npy')

print(f"  í•™ìŠµ: {len(X_train):,}, í…ŒìŠ¤íŠ¸: {len(X_test):,}")

# ìƒ˜í”Œë§ (ë¹ ë¥¸ íŠœë‹ì„ ìœ„í•´)
sample_size = min(1000000, len(X_train))
sample_idx = np.random.choice(len(X_train), sample_size, replace=False)
X_train_sample = X_train[sample_idx]
y_train_sample = y_train[sample_idx]
print(f"  íŠœë‹ìš© ìƒ˜í”Œ: {len(X_train_sample):,}ê±´ (ë¹ ë¥¸ ì‹¤í—˜)")

# ============================================================
# 2. LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
# ============================================================
print("\n[2/5] LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")

def objective_lgb(trial):
    """
    Optuna Objective: Accuracyì™€ Macro F1 ê· í˜•ì¡íŒ ìµœì í™”
    """
    params = {
        'objective': 'multiclass',
        'num_class': 6,
        'metric': 'multi_logloss',
        'boosting_type': 'gbdt',
        'device': 'gpu',
        'gpu_platform_id': 0,
        'gpu_device_id': 0,
        
        # íŠœë‹ ëŒ€ìƒ íŒŒë¼ë¯¸í„°
        'num_leaves': trial.suggest_int('num_leaves', 64, 512),
        'max_depth': trial.suggest_int('max_depth', 8, 20),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'n_estimators': trial.suggest_int('n_estimators', 200, 600),
        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),
        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0.0, 1.0),
        
        'random_state': 42,
        'n_jobs': -1,
        'verbose': -1
    }
    
    # ëª¨ë¸ í•™ìŠµ
    model = lgb.LGBMClassifier(**params)
    model.fit(X_train_sample, y_train_sample)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€
    y_pred = model.predict(X_test)
    
    # Multi-objective: Accuracy 60% + Macro F1 40%
    accuracy = accuracy_score(y_test, y_pred)
    macro_f1 = f1_score(y_test, y_pred, average='macro')
    
    # ê· í˜•ì¡íŒ ì ìˆ˜
    balanced_score = 0.6 * accuracy + 0.4 * macro_f1
    
    # ìƒí™œ ì¹´í…Œê³ ë¦¬ F1ë„ ê³ ë ¤ (ë³´ë„ˆìŠ¤)
    category_f1 = f1_score(y_test, y_pred, average=None)
    life_f1 = category_f1[1]  # ìƒí™œ ì¹´í…Œê³ ë¦¬
    
    # ìƒí™œ F1ì´ 15% ì´ìƒì´ë©´ ë³´ë„ˆìŠ¤ ì ìˆ˜
    if life_f1 > 0.15:
        balanced_score += 0.01  # 1% ë³´ë„ˆìŠ¤
    
    return balanced_score

print("  Optuna ì‹œì‘ (50 trials)...")
print("  ëª©í‘œ: Accuracy 60% + Macro F1 40% ê· í˜• ìµœì í™”")

study_lgb = optuna.create_study(direction='maximize', study_name='lightgbm_gpu_balanced')
study_lgb.optimize(objective_lgb, n_trials=50, show_progress_bar=True)

print(f"\n  âœ… ìµœì  ì ìˆ˜: {study_lgb.best_value:.4f}")
print(f"  ğŸ“‹ ìµœì  íŒŒë¼ë¯¸í„°:")
for key, value in study_lgb.best_params.items():
    if isinstance(value, float):
        print(f"     {key}: {value:.6f}")
    else:
        print(f"     {key}: {value}")

# ============================================================
# 3. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ ë°ì´í„° í•™ìŠµ
# ============================================================
print("\n[3/5] ìµœì  ëª¨ë¸ í•™ìŠµ (ì „ì²´ ë°ì´í„°)")

best_params_lgb = study_lgb.best_params.copy()
best_params_lgb['objective'] = 'multiclass'
best_params_lgb['num_class'] = 6
best_params_lgb['metric'] = 'multi_logloss'
best_params_lgb['boosting_type'] = 'gbdt'
best_params_lgb['device'] = 'gpu'
best_params_lgb['gpu_platform_id'] = 0
best_params_lgb['gpu_device_id'] = 0
best_params_lgb['random_state'] = 42
best_params_lgb['n_jobs'] = -1
best_params_lgb['verbose'] = -1

print(f"\n  [LightGBM] ì „ì²´ ë°ì´í„° í•™ìŠµ ì‹œì‘...")
start = time.time()
best_lgb = lgb.LGBMClassifier(**best_params_lgb)
best_lgb.fit(X_train, y_train)
train_time = time.time() - start
print(f"  âœ… í•™ìŠµ ì™„ë£Œ: {train_time:.2f}ì´ˆ ({train_time/60:.2f}ë¶„)")

# í‰ê°€
y_pred = best_lgb.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
macro_f1 = f1_score(y_test, y_pred, average='macro')
weighted_f1 = f1_score(y_test, y_pred, average='weighted')
category_f1 = f1_score(y_test, y_pred, average=None)

print(f"\n  ğŸ“Š ìµœì¢… ì„±ëŠ¥:")
print(f"     Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"     Macro F1:    {macro_f1:.4f} ({macro_f1*100:.2f}%)")
print(f"     Weighted F1: {weighted_f1:.4f}")

category_names = ['êµí†µ', 'ìƒí™œ', 'ì‡¼í•‘', 'ì‹ë£Œí’ˆ', 'ì™¸ì‹', 'ì£¼ìœ ']
print(f"\n  ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ F1:")
for cat, f1 in zip(category_names, category_f1):
    print(f"     {cat:6s}: {f1:.4f}")

# ============================================================
# 4. ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥
# ============================================================
print("\n[4/5] ëª¨ë¸ ì €ì¥")

import os
os.makedirs('03_models/lightgbm_optuna', exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# ëª¨ë¸ ì €ì¥
model_path = f'03_models/lightgbm_optuna/lightgbm_tuned_{timestamp}.joblib'
joblib.dump(best_lgb, model_path)
print(f"  âœ… ëª¨ë¸ ì €ì¥: {model_path}")

# ë©”íƒ€ë°ì´í„° ì €ì¥
metadata = {
    'model_info': {
        'name': 'LightGBM GPU (Optuna Tuned - Balanced)',
        'tuning_method': 'Optuna TPE',
        'n_trials': 50,
        'objective': 'Balanced: 60% Accuracy + 40% Macro F1',
        'created_at': datetime.now().isoformat()
    },
    'best_params': best_params_lgb,
    'performance': {
        'accuracy': round(accuracy, 4),
        'macro_f1': round(macro_f1, 4),
        'weighted_f1': round(weighted_f1, 4),
        'category_f1': {cat: round(f1, 4) for cat, f1 in zip(category_names, category_f1)},
        'train_time_seconds': round(train_time, 2)
    },
    'comparison': {
        'baseline_lgb_acc': 0.4913,
        'baseline_lgb_f1': 0.4344,
        'improvement_acc': round((accuracy - 0.4913) * 100, 2),
        'improvement_f1': round((macro_f1 - 0.4344) * 100, 2)
    }
}

metadata_path = f'03_models/lightgbm_optuna/metadata_{timestamp}.json'
with open(metadata_path, 'w', encoding='utf-8') as f:
    json.dump(metadata, f, indent=2, ensure_ascii=False)
print(f"  âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")

# ============================================================
# 5. ê²°ê³¼ ë¹„êµ
# ============================================================
print("\n" + "="*80)
print("ğŸ† LightGBM Optuna íŠœë‹ ê²°ê³¼")
print("="*80)

baseline = {'accuracy': 0.4913, 'macro_f1': 0.4344, 'life_f1': 0.0802}

print(f"\n{'ëª¨ë¸':<40} {'Accuracy':>12} {'Macro F1':>12} {'ìƒí™œ F1':>12}")
print("-"*80)
print(f"{'ê¸°ì¡´ LightGBM (Baseline)':<40} {baseline['accuracy']:>12.4f} {baseline['macro_f1']:>12.4f} {baseline['life_f1']:>12.4f}")
print(f"{'Optuna LightGBM (NEW)':<40} {accuracy:>12.4f} {macro_f1:>12.4f} {category_f1[1]:>12.4f}")
print("-"*80)

acc_improve = (accuracy - baseline['accuracy']) * 100
f1_improve = (macro_f1 - baseline['macro_f1']) * 100
life_improve = (category_f1[1] - baseline['life_f1']) * 100

print(f"\nğŸ“Š ê°œì„ ë„:")
print(f"  Accuracy:    {acc_improve:+.2f}%p")
print(f"  Macro F1:    {f1_improve:+.2f}%p")
print(f"  ìƒí™œ F1:     {life_improve:+.2f}%p")

if accuracy > baseline['accuracy'] and macro_f1 > baseline['macro_f1']:
    print(f"\nâœ… ì„±ëŠ¥ ê°œì„  ì„±ê³µ! ëª¨ë“  ì§€í‘œ í–¥ìƒ")
elif accuracy > baseline['accuracy'] or macro_f1 > baseline['macro_f1']:
    print(f"\nâœ… ë¶€ë¶„ ê°œì„  ì„±ê³µ!")
else:
    print(f"\nâš ï¸ ì¶”ê°€ íŠœë‹ í•„ìš”")

print("\n" + "="*80)
print("âœ… LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ!")
print("="*80)
print(f"\nğŸ“¦ ì €ì¥ íŒŒì¼:")
print(f"   - ëª¨ë¸: {model_path}")
print(f"   - ë©”íƒ€ë°ì´í„°: {metadata_path}")
print("\n" + "="*80)
